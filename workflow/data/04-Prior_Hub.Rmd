---
title: "04-Prior_Hub"
output: github_document
---


```{r}
library(MASS)
library(Matrix)
library(igraph)
library(huge)
library(pulsar)


generator_Hub <- function (n, p, rho, g, vis, verbose) 
{
    gcinfo(FALSE)
    if (verbose) 
        cat("Generating data from the multivariate normal distribution with the Hub graph structure....")
  
  g.large = p%%g #Rest Funktion
    g.small = g - g.large
    n.small = floor(p/g)
    n.large = n.small + 1
    g.list = c(rep(n.small, g.small), rep(n.large, g.large))
    g.ind = rep(c(1:g), g.list)
    rm(g.large, g.small, n.small, n.large, g.list)
    gc()
    
    theta = matrix(0, p, p) #Here Theta defined (pxp matrix with entries "0")
  
    for (i in 1:g) {
        tmp = which(g.ind == i)
        theta[tmp[1], tmp] = 1
        theta[tmp, tmp[1]] = 1
        rm(tmp)
        gc()
    }
  
    diag(theta) = 0
    omega = theta * rho
    diag(omega) = 1 #Set diagonal of precision matrix to 1 (Liu et al.)
    sigma = solve(omega) #Knackpunkt! Das ist nun das Sigma, welche zu unserem simulierten theta gehÃ¶rt.
    x = mvrnorm(n, rep(0, p), sigma) #Dieses Sigma wird schlussendlich verwendet, um die Daten zu simulieren!!!
    sigmahat = cov(x) #Empirical covariance matrix
    
    if (vis == TRUE) {
        fullfig = par(mfrow = c(2, 2), pty = "s", omi = c(0.3, 
            0.3, 0.3, 0.3), mai = c(0.3, 0.3, 0.3, 0.3))
        fullfig[1] = image(theta, col = gray.colors(256), main = "Adjacency Matrix")
        fullfig[2] = image(sigma, col = gray.colors(256), main = "Covariance Matrix")
        g = graph.adjacency(theta, mode = "undirected", diag = FALSE)
        layout.grid = layout.fruchterman.reingold(g)
        fullfig[3] = plot(g, layout = layout.grid, edge.color = "gray50", 
            vertex.color = "red", vertex.size = 3, vertex.label = NA, 
            main = "Graph Pattern")
        fullfig[4] = image(sigmahat, col = gray.colors(256), 
            main = "Empirical Matrix")
        rm(fullfig, g, layout.grid)
        gc()
    }
    if (verbose) 
        cat("done.\n")
    rm(vis, verbose)
    gc()
    sim = list(data = x, sigma = sigma, sigmahat = sigmahat, 
        omega = omega, theta = as(as(as(theta, "lMatrix"), "generalMatrix"), "CsparseMatrix"),
        act_sparsity = sum(theta)/(p * (p - 1))) #-1 because every node can connect to p-1 nodes (discarding diag)
    class(sim) = "sim"
    return(sim)
}


n <- 800 # Samples
p <- 40 # Dimensions
s <- 20    # Size Hub Group
J <- floor(p/s) # Number of Hubs
b = ifelse(n > 144, (floor(10*sqrt(n)))/n, 0.8) # Size Subsamples (Ratio)
N = 20 # Number of Repetitions
rho <- 0.20 # Off-Diagonal Effect Strength

Hub <- generator_Hub(n = n, p = p, rho = rho, g = J, vis = FALSE, verbose = TRUE)
Hub_data <- Hub$data
true_graph <- Hub$theta
act_sparsity <- Hub$act_sparsity

maxCov <- getMaxCov(Hub_data)
lambda_path  <- getLamPath(max = maxCov, min = 0.01, len = 30) #,log = TRUE
lambda <- list(lambda=lambda_path)

print('Condition number: ')
print(kappa(Hub$omega))

# Using QUIC
  library(BigQuic)
  quicr <- function(data, lambda) {
      p <- ncol(data)
      est  <- BigQuic(X = data, lambda=lambda, epsilon=1e-2, use_ram=TRUE, seed = NULL)
      est <- setNames(lapply(ls(envir=est), mget, envir=attr(unclass(est), '.xData')), ls(envir=est))
      path <-  lapply(seq(length(lambda)), function(i) {
                  tmp <- est$precision_matrices[[1]][[i]][1:p,1:p]
                  diag(tmp) <- 0
                  as(tmp!=0, "lMatrix")
      })
      est$path <- path
      est
  }
  
  # Using Huge GLASSO
  #library(huge)
  #huger <- function(data, lambda) {
    #est  <- huge::huge(data, lambda = lambda, method = "glasso")
    #path <- lapply(seq(length(lambda)), function(i) {
      ## convert precision array to adj list
      #tmp <- est$path[[i]]
      #tmp <- as(as(as(tmp, "lMatrix"), "generalMatrix"), "CsparseMatrix")
      #return(tmp)
    #})
    #est$path <- path
    #est
  #}
    

quicargs <- list(lambda = lambda_path)
#hugeargs <- list(lambda = lambda_path, method = "glasso", verbose = FALSE)
  
library(batchtools)
library(pulsar)
out.p <- my.batch.pulsar(
  Hub_data, 
  fun = quicr, 
  fargs = quicargs, 
  rep.num = N,
  thresh = 0.1,
  subsample.ratio = b,
  criterion=c('stars', 'gcd'), 
  lb.stars = TRUE, 
  ub.stars = TRUE, 
  seed = NULL,
  refit = FALSE,
  fullgcd = TRUE)


# Optimal Index Stars
stars_index <-  opt.index(out.p, 'stars')
# Optimal Lambda
best_lambda_stars <- round(lambda_path[stars_index], 3)
# Lower Bound
stars_lb <- out.p[["stars"]][["lb.index"]]
# Upper Bound
stars_ub <- out.p[["stars"]][["ub.index"]]

# Optimal Index Gstars
opt.index(out.p, criterion = "gcd") <- get.opt.index(out.p, criterion = "gcd")
gstars_index <- opt.index(out.p, 'gcd')
# Optimal Lambda
best_lambda_gstars <- round(lambda_path[gstars_index], 3)


fit  <- refit(out.p, criterion = c("stars", "gcd"))
## Stars
stars_graph <- fit[["refit"]][["stars"]]
## GStars
gstars_graph <- fit[["refit"]][["gcd"]]

plot(out.p, legends = F, scale = F)

## Oracle procedure
oracle_results <- quicr(Hub_data, lambda_path)

#F1-Score as criterium for Oracle
f1_score <- function(actual, predicted) {
if (!inherits(actual, "lgCMatrix") || !inherits(predicted, "lgCMatrix")) {
  stop("Matrices should be of class lgCMatrix")
}

# Calculating TP, FP, and FN using sparse matrix operations
TP <- sum(predicted & actual)
FP <- sum(predicted & !actual)
FN <- sum(!predicted & actual)

# Calculate Precision and Recall
Precision <- ifelse(TP + FP > 0, TP / (TP + FP), 0)
Recall <- ifelse(TP + FN > 0, TP / (TP + FN), 0)

# Calculate F1 Score
f1 <- ifelse(Precision + Recall > 0, 2 * (Precision * Recall) / (Precision + Recall), 0)

return(f1)
}

# Hamming distance as criterium for Oracle
hamming_distance <- function(actual, predicted) {
  sum(tril(predicted) != tril(actual))
}

# F1 score - best lambda Oracle
oracle_index_f1 <- which.max(sapply(1:length(lambda_path), function(j) {
  estimated_graph <- oracle_results$path[[j]]
  f1_score(true_graph, estimated_graph)
}))

# Hamming distance - best lambda Oracle
  oracle_index_hamming <- which.min(sapply(1:length(lambda_path), function(j) {
  estimated_graph <- oracle_results$path[[j]]
  hamming_distance(true_graph, estimated_graph)
}))

best_lambda_oracle_f1 <- round(lambda_path[oracle_index_f1], 3)
best_lambda_oracle_hamming <- round(lambda_path[oracle_index_hamming], 3)

oracle_graph_f1 <- oracle_results$path[[oracle_index_f1]]
oracle_graph_hamming <- oracle_results$path[[oracle_index_hamming]]


```

## variability plot
```{r}
d_hat <- out.p[["fullgcd"]][["summary"]]
d_hat
D_hat <- out.p[["stars"]][["summary"]]
D_hat
lambda_path
thresh <- out.p[["stars"]][["thresh"]]
stars_index
gstars_index
stars_lb
stars_ub


# Load ggplot2
library(ggplot2)

# Normalize d_hat and D_hat to range [0, 1]
normalize <- function(x) {
    (x - min(x)) / (max(x) - min(x))
}

d_hat_norm <- normalize(d_hat)
D_hat_norm <- normalize(D_hat)

# Create a data frame for plotting
data <- data.frame(
  lambda = lambda_path,
  d_hat = d_hat_norm,
  D_hat = D_hat_norm
)


# Use ggplot to create the plot with additional vertical lines, points, and annotations
ggplot(data) +
  geom_line(aes(x = lambda, y = d_hat, colour = "d_hat"), size = 1.3) +
  geom_line(aes(x = lambda, y = D_hat, colour = "D_hat"), size = 1.3) +
  geom_hline(yintercept = thresh, linetype = "dashed", color = "black", linewidth = 1) +
  geom_vline(xintercept = lambda_path[stars_lb], linetype = "dotted", color = "gray", size = 1) +
  geom_vline(xintercept = lambda_path[stars_ub], linetype = "dotted", color = "gray", size = 1) +
  geom_point(aes(x = lambda_path[gstars_index], y = d_hat_norm[gstars_index]), color = "#0066cc", size = 2) +
  geom_point(aes(x = lambda_path[stars_index], y = D_hat_norm[stars_index]), color = "#cc3333", size = 2) +
  scale_colour_manual("", 
                      breaks = c("d_hat", "D_hat"),
                      labels = c(expression(italic(bold(hat(d)))), expression(italic(bold(bar(D))))),
                      values = c("d_hat" = "#0066cc", "D_hat" = "#cc3333")) +
  labs(x = expression(italic(bold(lambda[k]))), 
       y = "Variability", 
       title = expression(italic(bold("Normalized Variability vs Lambda")))) +
  theme_minimal() +
  theme(text = element_text(size = 12),  # Adjust text size globally
        axis.title = element_text(size = 14),  # Adjust axis title size
        plot.title = element_text(size = 16, face = "bold.italic"))  # Adjust plot title size and style
             





```


## Arriving at gcd
```{r}
#n: number of nodes in a network

## Step 1: Grphlet Degree Vector gdv (1x11)
#skipped


## Step 2: Graphlet Degree Matrix gdm (nx11)
# Converting Hub adjacency matrix to an nx2 edge matrix
true_graph
stars_graph
stars_graph@x
# Count nodes
sum(tril(true_graph))
sum(tril(stars_graph))
true_edge <- which(tril(true_graph, diag = TRUE) == TRUE, arr.ind=TRUE) 
stars_edge <- which(tril(stars_graph, diag = TRUE) == TRUE, arr.ind=TRUE) 
true_edge # 20 hubs*19 connected node = 380/2 = 190 edges
stars_edge
# Sparse alternative from .adj2elist
# (Elist <- .adj2elist(true_graph))
# (Elist <- .adj2elist(stars_graph))
# Count nodes
sum(nrow(true_edge)) # only 190 for p = 200, because diag elements = 0 in adjacency matrix
sum(nrow(stars_edge))
# Function to convert adjacency matrix to nx2 edge matrix, considering only the lower triangle
# convert_to_edge_matrix <- function(adj_matrix) {
#   edges <- which(tril(adj_matrix) == 1, arr.ind = TRUE)
#   edge_matrix <- as.matrix(edges)
#   colnames(edge_matrix) <- c("Node1", "Node2")
#   return(edge_matrix)
# }
# 
# convert_to_edge_matrix(true_graph)

# Determine graphlets via Orca 
library(orca)
true_graphlets <- count4(true_edge)
stars_graphlets <- count4(stars_edge)
true_graphlets 
stars_graphlets
# Select 11 non-redundant orbits
true_non_redun <- true_graphlets[, c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1) +1]
stars_non_redun <- stars_graphlets[, c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1) +1]
true_non_redun #nx11
stars_non_redun
true_non_redun <- rbind(true_non_redun, 1) # Avoid error for std = 0, by setting all zero colomns to 1
stars_non_redun <- rbind(stars_non_redun, 1) 
# Each row in this matrix corresponds to a node in the graph, and each column represents a different orbit type within 4-node graphlets. 
# The values in the matrix are the counts of how many times a node participates in a particular orbit.



## Step 3: Graphlet Correlation Matrix gcm (11x11)
# Calculate the Graphlet Correlation Matrix (GCM) using Spearman's correlation
# Measuring the pairwise correlation between each type of graphlet orbit across all nodes.
gcm_true <- suppressWarnings(cor(true_non_redun, method = "spearman"))
gcm_stars <- cor(stars_non_redun, method = "spearman")
gcm_true 
gcm_stars



## Step 4: Graphlet Correlation Vector gcv
gcv_stars <- gcm_stars[upper.tri(gcm_stars)]
gcv_true <- gcm_true[upper.tri(gcm_true)]
gcv_stars # 1 for fake edges
gcv_true


library(pulsar)
gcv_true_test <- gcvec(true_graph, orbind=c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1)+1)
gcv_stars_test <- gcvec(stars_graph, orbind=c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1)+1)

gcv_true_test
gcv_stars_test



## Step 5: Graphlet Correlation Distance gcd

# Function to calculate total graphlet variability measure over N graphs for each lambda
calculate_total_graphlet_variability <- function(gcv_list) {
  n <- length(gcv_list)
  # Initialize a numeric vector to store the total variability measure for each lambda
  d_N_vector <- numeric(n)
  
  for (k in 1:n) {
    # Compute all pairwise Euclidean distances for the k-th lambda
    pairwise_distances <- combn(gcv_list[[k]], 2, function(gcv_pair) {
      sqrt(sum((gcv_pair[[1]] - gcv_pair[[2]])^2))
    })
    # Calculate the total graphlet variability measure for the k-th lambda
    d_N_vector[k] <- 2 / (n * (n - 1)) * sum(pairwise_distances)
  }
  
  return(d_N_vector)
}

# Example usage:
d_N_vector <- calculate_total_graphlet_variability(gcv_list)
d_N_vector  # This will return the total graphlet variability measure for each lambda



    
  # Graphlet stability path
  d_hat <- out.p$gcd$summary
  d_hat_scaled <- d_hat / max(d_hat)
  
  D_hat
  d_hat
  
  out.p[["gcd"]][["merge"]][[23]] # Already dist of gcv
  (2/(N*(N-1)))*sum(out.p[["gcd"]][["merge"]][[23]]) # d_hat
  
  
  (hui <- fit[["est"]][["path"]][[23]])
  (hui2 <- gcvec(hui, orbind = c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1) + 1))
  (hui3 <- dist(hui2, method = "euclidean"))
  (hui4 <- (2/(N*(N-1)))*sum(hui3))
  
  hui <- triu(fit[["est"]][["path"]][[23]])
  hui
  test <- dist(gcvec(hui, orbind = c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1) + 1))
  test <- dist(gcvec_extended(hui, orbind = c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1) + 1)) # dist of gcv
  test
  (2/(N*(N-1)))*sum(test)

  est$merge <- lapply(premerge, function(pm) dist(t(sapply(pm, gcvec))))

```



## Debugging batch.pulsar
```{r}

my.batch.pulsar <- function(data, fun=huge::huge, fargs=list(),
                    criterion=c("stars"), thresh = 0.1, subsample.ratio = NULL,
                    lb.stars=FALSE, ub.stars=FALSE, rep.num = 20, seed=NULL,
                    wkdir=getwd(), regdir=NA, init="init", conffile='',
                    job.res=list(), cleanup=FALSE, refit=TRUE, fullgcd = FALSE) {

    if (!requireNamespace('batchtools', quietly=TRUE)) {
        stop("'batchtools' package required to run 'batch.pulsar'")
    }
    gcinfo(FALSE)
    if (!is.na(regdir) && file.exists(regdir)) {
        stop('Registry directory already exists')
    }

    n <- nrow(data)
    p <- ncol(data)
    knowncrits <- c("stars", "gcd", "estrada", "sufficiency")
    .lamcheck(fargs$lambda)
    .critcheck0(criterion, knowncrits)
    subsample.ratio <- .ratcheck(subsample.ratio, n)
    nlams <- length(fargs$lambda)
    conffile <- findConfFile(conffile)

    if (!is.null(seed)) set.seed(seed)
    ind.sample <- replicate(rep.num, sample(c(1:n), floor(n * subsample.ratio), replace = FALSE), simplify = FALSE)
    if (refit) {
        tmp <- 1L:n
        attr(tmp, 'full') <- TRUE
        ind.sample <- c(list(tmp), ind.sample)
    }
    if (!is.null(seed)) set.seed(NULL)

    estFun <- function(ind.sample, fargs, data, fun) {
        tmp <- do.call(fun, c(fargs, list(data[ind.sample,])))
        if (!('path' %in% names(tmp))) {
            stop('Error: expected data structure with \'path\' member')
        }
        if (isTRUE(attr(ind.sample, 'full'))) {
            return(tmp)
        } else {
            return(tmp$path)
        }
    }

    est <- list()
    reduceargs <- list()
    reduceGCDargs <- list()
    
    ################
    
    if ("gcd" %in% criterion) {
      if (fullgcd) {

      minN <- rep.num + refit
      isamp <- ind.sample[1:minN] # selects the first minN subsamples # ind.sample is a list of indices for subsampling the data
      out <- batchply(data, estFun, fun, fargs, isamp, wkdir, regdir, conffile, job.res)
      reg <- out$reg # extract the registry object
      id <- out$id # and job ids from the output of batchply. # manage and track the submitted batch jobs.
      doneRun <- batchtools::waitForJobs(reg = reg, id) # waits for all submitted jobs to finish.
      jdone <- batchtools::findDone(reg = reg, id) # Identifies which jobs have completed successfully.
      pulsar.jobs <- intersect((1 + refit):minN, jdone$job.id) # ids of jobs successfully completed among initially submitted jobs.
  
      gcdaggfun_compl <- function(res) lapply(res, gcvec) # Function to calculate gcv
      gcdpremerge_compl <- c(reduceGCDargs$init,
                       batchtools::reduceResultsList(reg = reg, ids = pulsar.jobs, fun = gcdaggfun_compl))
  
      gcdmerge_compl <- lapply(1:nlams, function(i) dist(t(sapply(1:rep.num, function(j) gcdpremerge_compl[[j]][[i]]))))
  
      est$fullgcd <- gcd.stability(NULL, thresh, rep.num, p, nlams, gcdmerge_compl)
      }
    }
    
    #############
    
    
    if (lb.stars) {
        if (!("stars" %in% criterion)) {
            stop('Lower/Upper bound method must be used with StARS')
        }
        minN <- 2 + refit # minimum number of subsamples for a meaningful StARS computation
        if (!is.na(regdir)) regdir <- paste(regdir, init, sep = "_")
    } else {
        minN <- rep.num + refit # minimum number of subsamples is set to the number of repetitions
    }
    
    
    # In summary, this code segment is crucial for initializing variables and setting up the function's environment, especially 
    # when dealing with the StARS criterion and its lower/upper bound methods. It determines how many subsamples are needed 
    # based on the selected criteria and function arguments.
    

    isamp <- ind.sample[1:minN] # selects the first minN subsamples # ind.sample is a list of indices for subsampling the data
    out <- batchply(data, estFun, fun, fargs, isamp, wkdir, regdir, conffile, job.res) # function estFun applies the statistical method (fun) with arguments (fargs) to the subsampled data (isamp). 
    reg <- out$reg # extract the registry object
    id <- out$id # and job ids from the output of batchply. # manage and track the submitted batch jobs.
    doneRun <- batchtools::waitForJobs(reg = reg, id) # waits for all submitted jobs to finish.
    jdone <- batchtools::findDone(reg = reg, id) # Identifies which jobs have completed successfully.
    pulsar.jobs <- intersect((1 + refit):minN, jdone$job.id) # ids of jobs successfully completed among initially submitted jobs.

    if (refit) { #block checks if there is a need to refit the model using the full dataset (not just subsamples).
        fullmodel <- batchtools::loadResult(id = 1, reg = reg) # If refit is TRUE, loads the result of the full dataset fit.
        minN <- minN - 1L # Adjusts the count of the number of subsamples by subtracting one, accounting for the full dataset fit.
    } else {
        fullmodel <- NULL #  indicating that no full dataset fit was performed.
    }
    
    # In essence, this section of code is central to the functioning of my.batch.pulsar, as it handles the parallel execution of
    # model estimation over subsamples of data, waits for these jobs to complete, and collects the results for further analysis. 
    # It also anages the optional refit of the model using the full dataset.
    

    
    starsaggfun <- function(res, aggr) lapply(1:length(aggr), function(i) aggr[[i]] + res[[i]])
    # This is a custom function defined to aggregate the results of the subsampled analyses.
    
    
    if (lb.stars) {
        est$init.reg <- reg # Store the registry and job IDs.
        est$init.id <- id

        if (!doneRun) {
            stop('Errors in batch jobs for computing initial stability')
        }

        lb.starsmerge <- batchtools::reduceResults(reg = reg, ids = pulsar.jobs, fun = starsaggfun)
        # Aggregates the results of the initial set of jobs (for StARS method) using reduceResults
        lb.est <- stars.stability(NULL, thresh, minN, p, lb.starsmerge) 
        # Calculates the stability of the lower bound estimate using the aggregated results.

        if ('gcd' %in% criterion) { # lb.gcdpremerge aggregates pre-reduction results for GCD, useful for later calculations. !!!
            aggfun <- function(job, res) lapply(res, gcvec) #  wrapper to apply the gcvec function to each result of each job.
            lb.gcdpremerge <- do.call(batchtools::reduceResultsList, # function that aggregates results from multiple jobs.
                                      c(list(reg = reg, ids = pulsar.jobs, fun = aggfun), reduceGCDargs))
            
            rep.num <- length(pulsar.jobs)
            nlams <- length(fargs$lambda)
            lb.gcdmerge <- lapply(1:nlams, function(i) dist(t(sapply(1:rep.num, function(j) lb.gcdpremerge[[j]][[i]]))))
            est$lb.est_gcd <- lb.gcdmerge 
            
            # The use of c(list(...), reduceGCDargs) combines the standard arguments with the additional ones into a single list, 
            # which is then passed to do.call.
        }

        if (cleanup) unlink(reg$file.dir, recursive = TRUE) # deletes the temporary files created during the batch job execution.
        if (lb.est$opt.index == 1) {
            warning("Accurate lower bound could not be determined with the first 2 subsamples")
        }
        
        # upper bound is determined by equivalent of maximum entropy of Poisson Binomial
        if (ub.stars) { # This block calculates the upper bound index for lambda selection, based on the variability of the results.
            pmean <- sapply(lb.est$merge, function(x) sum(x) / (p * (p - 1))) # calculates the mean probability of edge inclusion.
            ub.summary <- cummax(4 * pmean * (1 - pmean)) # Determine the index at which the variability 
            tmpub <- .starsind(ub.summary, thresh, 1) # of results is below the threshold.
            ub.index <- if (any(ub.summary == 0)) { # Sets the index for the upper bound.
                max(tmpub, max(which(ub.summary == 0)) + 1)
            } else {
                max(tmpub, 1)
            }
        } else {
            ub.index <- 1
        }
        

        fargs$lambda <- fargs$lambda[ub.index:lb.est$opt.index] # Adjusts the lambda path to be within the bounds !!!!!!!
        nlams <- length(fargs$lambda)
        reduceargs <- list(init = lb.starsmerge[ub.index:lb.est$opt.index]) 
        # Prepare arguments for reducing results in the subsequent batch jobs.
        
        # In summary, this chunk is crucial for conducting stability selection with optional lower and upper bound determination. 
        # It aggregates results from initial subsamples, adjusts the lambda path according to these bounds, 
        # executes additional jobs, and prepares for the final aggregation and analysis of results.
        
        

        if ('gcd' %in% criterion) { # HIER WIRD REDUZIERT !!!!!
            reduceGCDargs <- list(init = lapply(lb.gcdpremerge, function(gcdpm) gcdpm[ub.index:lb.est$opt.index])) 
            
            # takes the pre-merged results (lb.gcdpremerge) and selects a subset of them based on the indices specified by  
            # ub.index (upper bound index) and lb.est$opt.index (optimal index from lower bound estimation). 
            # This subset of results will be used in subsequent calculations.
        }
        
        regdir <- gsub(paste("_", init, sep = ""), "", regdir)
        isamp <- ind.sample[-(1:minN)] # First minN subsamples were already processed, and the remaining ones are to be handled now.
        out <- batchply(data, estFun, fun, fargs, isamp, wkdir, regdir, conffile, job.res) 
        # batchply function is called to submit batch jobs for processing
        reg <- out$reg
        id <- out$id
        doneRun <- batchtools::waitForJobs(reg = reg, id)
        jdone <- batchtools::findDone(reg = reg, id)
        pulsar.jobs <- intersect((1 + refit):rep.num, jdone$job.id)
    }

    rep.num <- length(pulsar.jobs)
    if (lb.stars) rep.num <- rep.num + minN
    if (!doneRun) {
        warning(paste("Only", length(jdone), "jobs completed... proceeding anyway"))
    }

    for (i in 1:length(criterion)) {
        crit <- criterion[i]
        if (crit == "stars") {
            starsmerge <- do.call(batchtools::reduceResults,
                                  c(list(reg = reg, ids = pulsar.jobs, fun = starsaggfun), reduceargs))
            
            est$stars <- stars.stability(NULL, thresh, rep.num, p, starsmerge)
        }

        if (crit == "gcd") {
            gcdaggfun <- function(res) lapply(res, gcvec) # Function to calculate gcv
            gcdpremerge <- c(reduceGCDargs$init,
                             batchtools::reduceResultsList(reg = reg, ids = pulsar.jobs, fun = gcdaggfun))
            
            gcdmerge <- lapply(1:nlams, function(i) dist(t(sapply(1:rep.num, function(j) gcdpremerge[[j]][[i]]))))
            
            est$gcd <- gcd.stability(NULL, thresh, rep.num, p, nlams, gcdmerge)
            
            
        } else if (crit == "estrada") {
            if (!("stars" %in% criterion)) {
                warning('Need StaRS for computing Estrada classes... not run')
            } else {
                est$estrada <- estrada.stability(est$stars$merge, thresh, rep.num, p, nlams)
            }
        } else if (crit == "sufficiency") {
            if (!("stars" %in% criterion)) {
                warning('Need StaRS for computing sufficiency... not run')
            } else {
                est$sufficiency <- sufficiency(est$stars$merge, rep.num, p, nlams)
            }
        }
    }

    if (lb.stars) {
        pind <- ub.index:lb.est$opt.index
        pinv <- setdiff(1:length(lb.est$summary), pind)
        tmpsumm <- vector('numeric', length(lb.est$summary))
        tmpsumm[pinv] <- lb.est$summary[pinv]
        tmpsumm[pind] <- est$stars$summary
        est$stars$summary <- tmpsumm

        tmpmerg <- vector('list', length(lb.est$summary))
        tmpmerg[pinv] <- lb.est$merge[pinv]
        tmpmerg[pind] <- est$stars$merge
        est$stars$merge <- tmpmerg

        est$stars$lb.index <- lb.est$opt.index
        est$stars$ub.index <- ub.index
        est$stars$opt.index <- est$stars$opt.index + ub.index - 1
    }

    if (cleanup) unlink(reg$file.dir, recursive = TRUE)
    est$id <- id
    est$reg <- reg
    est$call <- match.call()
    est$est <- fullmodel
    est$envir <- parent.frame()


    print("Completed batch.pulsar function.")
    return(structure(est, class = c("batch.pulsar", "pulsar")))
}



```




## Helper Functions for batch.pulsar
```{r}

library(batchtools)
library(pulsar)

findConfFile <- function(name='') {
 ## if x is not a file
 ## look for config file using batchtools rules,
 ## otherwise, look in the pulsar system package

  conffile <- batchtools::findConfFile()
  if (!is.na(conffile)) return(conffile)

  if (checkmate::testFileExists(name, access = "r"))
    return(fs::path_real(name))

  ## append type to file extension for default config files
  if (nchar(name)==0) name <- '.R'
  else name <- paste0('.', tools::file_path_sans_ext(name), '.R')

  conffile <- fs::path_real(system.file('config',
                  sprintf('batchtools.conf%s', name), package='pulsar'))
  # }
  if (checkmate::testFileExists(conffile, access = "r")) return(conffile)
  else return(character(0))
}

#' @keywords internal
.lamcheck <- function(lams) {
    if (is.null(lams)) {
        stop(paste('Error: missing members in fargs:',
             paste(c('lambda')[c(is.null(lams))])))
    } else {
        if (!all(lams == cummin(lams)))
            warning("Are you sure you don't want the lambda path to be monotonically decreasing")
        if (length(lams) < 2)
            warning("Only 1 value of lambda is given. Are you sure you want to do model selection?")
    }
}

#' @keywords internal
.ratcheck <- function(subsample.ratio, n) {
    if (is.null(subsample.ratio)) {
        if (n > 144)
            return(10 * sqrt(n)/n)
        else
            return(0.8)
    } else return(subsample.ratio)
}

#' @keywords internal
.critcheck0 <- function(criterion, knowncrits) {
    if (!all(criterion %in% knowncrits)) {
       stop(paste('Unknown criterion', paste(criterion[!(criterion %in% knowncrits)],
                   collapse=", "), sep=": "))
    }
    starsdepend <- c("estrada", "sufficiency")
    if (any(starsdepend %in% knowncrits)) {
        if (any(starsdepend %in% criterion) && !("stars" %in% criterion)) {
             stop(paste('Criterion: ', paste(starsdepend[starsdepend %in% criterion],
                   collapse=", "), ' cannot be run unless stars is also a selected criterion', sep=""))
        }
    }

}

# premerge: pre-aggregated results from previous computations, used as a starting point for stability calculations.

#' @keywords internal
stars.stability <- function(premerge, stars.thresh, rep.num, p, merge=NULL) { # Only 5 arguments!!!
    if (is.null(stars.thresh)) stars.thresh <- 0.1
    est <- list()


    # do.call(batchtools::reduceResults,
    #                  c(list(reg=reg, fun=starsaggfun), reduceargs))

    if (is.null(merge)) {
      est$merge <- lapply(premerge, function(x) Reduce("+", x)) # Like sum(), but can also used for adding matrices etc.
      # If merge is not provided, the function aggregates the premerge results to get a consolidated view of 
      #the model's performance across all subsamples.
      gc() # flush
    } else est$merge <- merge
    
    est$summary <- rep(0, length(est$merge)) # empty array

    for (i in 1:length(est$merge)) { # The function then computes a stability measure for each model (or each level 
      # of regularization, if applicable). This involves calculating the variability of model selection across subsamples.
      est$merge[[i]] <- est$merge[[i]]/rep.num
      est$summary[i] <- 4 * sum(est$merge[[i]] * (1 - est$merge[[i]])) / (p * (p - 1)) # binomial probab for edge appearing
    }
    ## monotonize variability
    est$summary   <- cummax(est$summary)
    est$opt.index <- .starsind(est$summary, stars.thresh) # Based on computed stability measures and provided threshold, function identifies optimal point 
    est$criterion <- "stars.stability"
    est$thresh    <- stars.thresh
    return(est)
}

est$gcd <- gcd.stability(NULL, thresh, rep.num, p, nlams, gcdmerge)
#' @importFrom stats dist
#' @keywords internal
gcd.stability <- function(premerge, thresh, rep.num, p, nlams, merge=NULL) { # 6 argumets including nlams
    est <- list()
    
    if (is.null(merge)) {
        est$merge <- lapply(premerge, function(pm) dist(t(sapply(pm, gcvec))))
    } else est$merge <- merge
    

    est$summary <- vector('numeric', nlams) # set up empty vector
    for (i in 1:nlams) est$summary[i] <- mean(est$merge[[i]]) # fill summary i with mean over all subsamples rep.num N
    est$criterion <- "graphlet.stability"
    return(est)
}





#' @keywords internal
sufficiency <- function(merge, rep.num, p, nlams) {
## Merge solution from StARS
  est <- list()
  est$merge <- sapply(merge, function(x) apply(x*(1-x), 2, max)) # Find maximum value of variance-like measure for each column of x.
  est$summary <- colMeans(est$merge)
  est$criterion <- 'sufficiency'
  return(est)
}


#' @keywords internal
estrada.stability <- function(merge, thresh, rep.num, p, nlams) {
    est <- list()
    est$summary <- unlist(lapply(merge, function(x) estrada.class(x >= .05)))
    if (!is.null(thresh))
      est$opt.index <- max(which.max(est$summary >= thresh)[1] - 1, 1)
    else
      est$opt.index <- 0

    est$criterion <- "estrada.stability"
    return(est)
}


#' @keywords internal
.starsind <- function(summary, thresh, offset=1) {
  if(any(summary >= thresh)){
    return(max(which.max(summary >= thresh)[1] - offset, 1))
  } else {
    warning("Optimal lambda may be outside the supplied values")
    return(length(summary))
  }
}


gcvec <- function(G, orbind=c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1)+1) {
  if (length(orbind) < 2) stop("Only one orbit selected, need at least two to calculate graphlet correlations")
  if (any(orbind > 15))   stop("Only 15 orbits, from 4-node graphlets, can be selected")
  Elist <- .adj2elist(G)
  n <- length(orbind)
  if (ncol(Elist) < 1 || nrow(Elist) < 1) {
      return(rep(0, n*(n-1)/2))
  }

  p <- ncol(G)
  gcount <- orca::count4(Elist)
  ## expand missing nodes
  buffer <- matrix(0, nrow=p-nrow(gcount), ncol=ncol(gcount))
  gcount <- rbind(gcount, buffer)
  ## warnings here are due to std dev == 0. This almost always occurs for a completely connected
  ## or completely empty graph and can be safely suppressed.
  gcor <- suppressWarnings(cor(rbind(gcount[,orbind],1), method='spearman'))
  gcor[upper.tri(gcor)]
}


#' @keywords internal
batchply <- function(data, estFun, fun, fargs, ind.sample, wkdir, regdir,
                     conffile, job.res) {
  reg <- batchtools::makeRegistry(file.dir=regdir, work.dir=wkdir,
                                  conf.file=findConfFile(conffile))
  args <- list(fargs=fargs, data=data, fun=fun)
  id   <- batchtools::batchMap(estFun, ind.sample, more.args=args, reg=reg)
  doneSub <- batchtools::submitJobs(reg=reg, resources=job.res)
  return(list(reg=reg, id=id))
}

#' @keywords internal
.adj2elist <- function(G) {
    if (inherits(G, "sparseMatrix")) {
        G <- Matrix::triu(G, k=1)
        index_i_j <- Matrix::mat2triplet(G)[1:2]
        return(as.data.frame(index_i_j))
    } else {
        p <- ncol(G)
        return(arrayInd(which(as.logical(triu(G))), c(p,p)))
    }
}

```


## Prior pulsar
```{r}
pulsar <- function(data, fun=huge::huge, fargs=list(),
                   criterion=c("stars"),
                   thresh = 0.1, subsample.ratio = NULL,
                   rep.num = 20, seed=NULL,
                   lb.stars=FALSE, ub.stars=FALSE,
                   ncores = 1, refit=TRUE, known_graph = FALSE, five = FALSE, orbind=c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1)+1)  {
  gcinfo(FALSE)
  n <- nrow(data)
  p <- ncol(data)
  # min requirements for function args
  .lamcheck(fargs$lambda)
  nlams <- length(fargs$lambda)
  knowncrits <- c("stars", "diss", "estrada", "gcd", "nc", "sufficiency", "gcd_new") #"vgraphlet", "egraphlet",
  .critcheck0(criterion, knowncrits)
  subsample.ratio <- .ratcheck(subsample.ratio, n)

  if (!is.null(seed)) set.seed(seed)
  ind.sample <- replicate(rep.num,
                  sample(c(1L:n), floor(n * subsample.ratio),
                  replace = FALSE), simplify=FALSE)
  if (refit) {
    tmp <- 1L:n
    attr(tmp, 'full') <- TRUE
    ind.sample <- c(list(tmp), ind.sample)
  }
  if (!is.null(seed)) set.seed(NULL)
  ## wrap estimator
  estFun <- function(ind.sample, fargs) {
    tmp <- do.call(fun, c(fargs, list(data[ind.sample,])))
    if (!('path' %in% names(tmp)))
      stop('Error: expected data stucture with \'path\' member')

    if (isTRUE(attr(ind.sample, 'full')))
      return(tmp)
    else
      return(tmp$path)
  }

  if (lb.stars) {
    if (!("stars" %in% criterion))
      stop('Lower/Upper bound method must be used with StARS')
    minN <- 2L + refit # Hard code for now
  } else minN <- rep.num + refit

  isamp <- ind.sample[1L:minN]
  ## don't pass on errors if lb.stars = TRUE
  premerge <- .try_mclapply(isamp, estFun, fargs = fargs, mc.cores = ncores,
                            mc.preschedule = FALSE, pass.errors = !lb.stars)
  errors <- attr(premerge, 'errors')
  # Adjust rep.num for failed jobs
  rep.num <- rep.num - ifelse(refit, sum(errors[-1]), sum(errors))

  if (refit) {
    fullmodel <- premerge[[1]]
    premerge  <- premerge[-1]
    minN <- minN - 1
  } else fullmodel <- NULL

  if (lb.stars) {
    lb.premerge       <- premerge
    lb.premerge.reord <- lapply(1L:nlams, function(i)
                         lapply(1L:minN, function(j) lb.premerge[[j]][[i]]))

    lb.est <- stars.stability(lb.premerge.reord, thresh, minN, p)
    if (lb.est$opt.index == 1)
      warning("Accurate lower bound could not be determined with the first 2 subsamples")
    if (ub.stars) {
      # upper bound is determined by equivilent of MaxEnt of Poisson Binomial
      pmean      <- sapply(lb.est$merge, function(x) { sum(x)/(p*(p-1)) })
      ub.summary <- cummax(4*pmean*(1-pmean))
      tmpub      <- .starsind(ub.summary, thresh, 1)
      if (any(ub.summary == 0))  ## adjust upper bound to exclude empty graphs
        ub.index <- max(tmpub, max(which(ub.summary == 0))+1)
      else
        ub.index <- max(tmpub, 1)
    } else ub.index <- 1
    # reselect lambda between bounds
    fargs$lambda <- fargs$lambda[ub.index:lb.est$opt.index]
    nlams <- length(fargs$lambda)
    lb.premerge  <- lapply(lb.premerge,
                           function(ppm) ppm[ub.index:lb.est$opt.index])
    isamp <- ind.sample[-(1L:(minN+refit))]
#    tmp   <- mclapply(isamp, estFun, fargs=fargs, mc.cores=ncores, mc.preschedule = FALSE)
    tmp <- .try_mclapply(isamp, estFun, fargs = fargs, mc.cores = ncores,
                         mc.preschedule = FALSE)
    # Adjust rep.num for failed jobs
    rep.num <- rep.num - sum(attr(tmp, 'errors'))
    premerge <- c(lb.premerge, tmp)
  }

  premerge.reord <- .tlist(premerge, nlams, rep.num)
  rm(premerge) ; gc()
  est <- list()

  for (i in 1L:length(criterion)) {
    crit <- criterion[i]

    if (crit == "stars")
      est$stars <- stars.stability(premerge.reord, thresh, rep.num, p)

    else if (crit == "diss")
      est$diss <-  diss.stability(premerge.reord, thresh, rep.num, p, nlams)

    else if (crit == "estrada") {
      if (!("stars" %in% criterion))
        warning('Need StaRS for computing Estrada classes... not run')
      else
        est$estrada <- estrada.stability(est$stars$merge,thresh,rep.num,p,nlams)
    }

    else if (crit == "sufficiency") {
      if (!("stars" %in% criterion)) warning('Need StaRS for computing sufficiency... not run')
      else  est$sufficiency <- sufficiency(est$stars$merge, rep.num, p, nlams)
    }

#      else if (crit == "egraphlet")
#        est$egraphlet <- egraphlet.stability(premerge.reord, thresh, rep.num, p, nlams)

#      else if (crit == "vgraphlet")
#        est$vgraphlet <- vgraphlet.stability(premerge.reord, thresh, rep.num, p, nlams)

    else if (crit == "gcd") {
      est$gcd <- gcd.stability(premerge.reord, thresh, rep.num, p, nlams)
      est$gcd$opt.index = est$gcd$opt.index + ub.index

    }else if (crit == "gcd_new"){
      est$gcd_new <- gcd.stability_extended(premerge.reord, thresh, rep.num, p, nlams, known_graph = known_graph, orbind = orbind, five = five)
      est$gcd_new$opt.index = est$gcd_new$opt.index + ub.index

    }else if (crit == "nc")
      est$nc <- nc.stability(premerge.reord, thresh, rep.num, p, nlams)

  }

  if (lb.stars) {
    find <- 1:length(lb.est$summary)
    pind <- ub.index:lb.est$opt.index
    pinv <- setdiff(find, pind)
    tmpsumm <- vector('numeric', length(lb.est$summary))
    tmpsumm[pinv] <- lb.est$summary[pinv]
    tmpsumm[pind] <- est$stars$summary
    est$stars$summary   <- tmpsumm

    tmpmerg <- vector('list', length(lb.est$summary))
    tmpmerg[pinv]   <- lb.est$merge[pinv]
    tmpmerg[pind]   <- est$stars$merge
    est$stars$merge <- tmpmerg

    est$stars$lb.index  <- lb.est$opt.index
    est$stars$ub.index  <- ub.index
    est$stars$opt.index <- est$stars$opt.index + ub.index - 1L
  }

  if ("stars" %in% criterion) {
      if (est$stars$opt.index == 1) {
          direction <- if (any(est$stars$summary >= .1)) "larger" else "smaller"
          warning(paste("Optimal lambda may be", direction, "than the supplied values"))
      }
  }
  est$call  <- match.call()
  est$envir <- parent.frame()
  est$est   <- fullmodel
  return(structure(est, class="pulsar"))
}

#' @keywords internal
.starsind <- function(summary, thresh, offset=1) {
    max(which.max(summary >= thresh)[1] - offset, 1)
}

#' @keywords internal
stars.stability <- function(premerge, stars.thresh, rep.num, p, merge=NULL) {
    if (is.null(stars.thresh)) stars.thresh <- 0.1
    est <- list()


    # do.call(batchtools::reduceResults,
    #                  c(list(reg=reg, fun=starsaggfun), reduceargs))

    if (is.null(merge)) {
      est$merge <- lapply(premerge, function(x) Reduce("+", x))
      gc() # flush
    } else est$merge <- merge
    est$summary <- rep(0, length(est$merge))

    for (i in 1:length(est$merge)) {
      est$merge[[i]] <- est$merge[[i]]/rep.num
      est$summary[i] <- 4 * sum(est$merge[[i]] * (1 - est$merge[[i]])) / (p * (p - 1))
    }
    ## monotonize variability
    est$summary   <- cummax(est$summary)
    est$opt.index <- .starsind(est$summary, stars.thresh)
    est$criterion <- "stars.stability"
    est$thresh    <- stars.thresh
    return(est)
}

#' @keywords internal
sufficiency <- function(merge, rep.num, p, nlams) {
## Merge solution from StARS
  est <- list()
  est$merge <- sapply(merge, function(x) apply(x*(1-x), 2, max))
  est$summary <- colMeans(est$merge)
  est$criterion <- 'sufficiency'
  return(est)
}

#' @keywords internal
.sumsq <- function(x2,y) x2 + y^2

#' @keywords internal
diss.stability <- function(premerge, diss.thresh, rep.num, p, nlams) {
    est <- list()
    disslist  <- lapply(premerge, function(pm) lapply(pm, graph.diss))
    est$merge <- lapply(disslist, function(dissmat) Reduce("+", dissmat)/rep.num)
    mergesq   <- lapply(disslist, function(dissmat)
                        Reduce(.sumsq, dissmat[-1], init=dissmat[[1]]^2)/rep.num)

    gc() # flush
    est$summary <- rep(0, length(est$merge))
    for (i in 1:length(est$merge)) {
        tmp <- mergesq[[i]] - est$merge[[i]]^2
        est$summary[i] <- sum(triu(tmp))  / (p * (p - 1))
    }
    est$mergesq <- mergesq
    est$criterion <- "diss.stability"
    return(est)
}


#estrada.stability <- function(premerge, thresh, rep.num, p, nlams) {
#    est <- list()
#    estrlist  <- lapply(premerge, function(pm) lapply(pm, estrada.class))
#    est$merge <- lapply(estrlist, function(x) table(unlist(x)))

##    gc() # flush
#    est$summary <- rep(0, length(est$merge))
#    for (i in 1:length(est$merge)) {
#        est$summary[i] <- 1-max(est$merge[[i]])/rep.num
#    }
#    ## monotonize variability
##    est$summary <- cummax(est$summary)
#    if (!is.null(thresh))
#      est$opt.index    <- max(which.max(est$summary >= thresh)[1] - 1, 1)
#    else
#      est$opt.index <- 0

#    est$criterion <- "estrada.stability"
#    return(est)
#}

#' @keywords internal
estrada.stability <- function(merge, thresh, rep.num, p, nlams) {
    est <- list()
    est$summary <- unlist(lapply(merge, function(x) estrada.class(x >= .05)))
    if (!is.null(thresh))
      est$opt.index <- max(which.max(est$summary >= thresh)[1] - 1, 1)
    else
      est$opt.index <- 0

    est$criterion <- "estrada.stability"
    return(est)
}

#' @keywords internal
nc.stability <- function(premerge, thresh, rep.num, p, nlams) {
    est <- list()
    est$merge <- sapply(premerge, function(x) sapply(x, natural.connectivity))
    est$summary <- colMeans(est$merge)
    est$criterion <- "nc.stability"
    return(est)
}

#' @importFrom stats dist
#' @keywords internal
gcd.stability <- function(premerge, thresh, rep.num, p, nlams, merge=NULL) {
    est <- list()
    if (is.null(merge))
        est$merge <- lapply(premerge, function(pm) dist(t(sapply(pm, gcvec))))
    else
        est$merge <- merge

    est$summary <- vector('numeric', nlams)
    for (i in 1:nlams) est$summary[i] <- mean(est$merge[[i]])
    est$criterion <- "graphlet.stability"
    est$opt.index <- max(which.min(est$summary)[1] - 1, 1)
    return(est)
}
```





