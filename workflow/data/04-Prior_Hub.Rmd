---
title: "04-Prior_Hub"
output: github_document
---


```{r}
library(MASS)
library(Matrix)
library(igraph)
library(huge)
library(pulsar)


generator_Hub <- function (n, p, rho, g, vis, verbose) 
{
    gcinfo(FALSE)
    if (verbose) 
        cat("Generating data from the multivariate normal distribution with the Hub graph structure....")
  
  g.large = p%%g #Rest Funktion
    g.small = g - g.large
    n.small = floor(p/g)
    n.large = n.small + 1
    g.list = c(rep(n.small, g.small), rep(n.large, g.large))
    g.ind = rep(c(1:g), g.list)
    rm(g.large, g.small, n.small, n.large, g.list)
    gc()
    
    theta = matrix(0, p, p) #Here Theta defined (pxp matrix with entries "0")
  
    for (i in 1:g) {
        tmp = which(g.ind == i)
        theta[tmp[1], tmp] = 1
        theta[tmp, tmp[1]] = 1
        rm(tmp)
        gc()
    }
  
    diag(theta) = 0
    omega = theta * rho
    diag(omega) = 1 #Set diagonal of precision matrix to 1 (Liu et al.)
    sigma = cov2cor(omega) #Knackpunkt! Das ist nun das Sigma, welche zu unserem simulierten theta gehÃ¶rt.
    x = mvrnorm(n, rep(0, p), sigma) #Dieses Sigma wird schlussendlich verwendet, um die Daten zu simulieren!!!
    sigmahat = cor(x) #Empirical covariance matrix
    
    if (vis == TRUE) {
        fullfig = par(mfrow = c(2, 2), pty = "s", omi = c(0.3, 
            0.3, 0.3, 0.3), mai = c(0.3, 0.3, 0.3, 0.3))
        fullfig[1] = image(theta, col = gray.colors(256), main = "Adjacency Matrix")
        fullfig[2] = image(sigma, col = gray.colors(256), main = "Covariance Matrix")
        g = graph.adjacency(theta, mode = "undirected", diag = FALSE)
        layout.grid = layout.fruchterman.reingold(g)
        fullfig[3] = plot(g, layout = layout.grid, edge.color = "gray50", 
            vertex.color = "red", vertex.size = 3, vertex.label = NA, 
            main = "Graph Pattern")
        fullfig[4] = image(sigmahat, col = gray.colors(256), 
            main = "Empirical Matrix")
        rm(fullfig, g, layout.grid)
        gc()
    }
    if (verbose) 
        cat("done.\n")
    rm(vis, verbose)
    gc()
    sim = list(data = x, sigma = sigma, sigmahat = sigmahat, 
        omega = omega, theta = as(as(as(theta, "lMatrix"), "generalMatrix"), "CsparseMatrix"),
        act_sparsity = sum(theta)/(p * (p - 1))) #-1 because every node can connect to p-1 nodes (discarding diag)
    class(sim) = "sim"
    return(sim)
}


n <- 400 # Samples
p <- 100 # Dimensions
s <- 20    # Size Hub Group
J <- floor(p/s) # Number of Hubs
b = ifelse(n > 144, (floor(10*sqrt(n)))/n, 0.8) # Size Subsamples (Ratio)
N = 20 # Number of Repetitions
rho <- 0.20 # Off-Diagonal Effect Strength

Hub <- generator_Hub(n = n, p = p, rho = rho, g = J, vis = FALSE, verbose = TRUE)
Hub_data <- Hub$data
true_graph <- Hub$theta
act_sparsity <- Hub$act_sparsity

maxCov <- getMaxCov(Hub_data)
lambda_path  <- getLamPath(max = maxCov, min = 0.01, len = 30) #,log = TRUE
lambda <- list(lambda=lambda_path)


# Using QUIC
  library(BigQuic)
  quicr <- function(data, lambda) {
      p <- ncol(data)
      est  <- BigQuic(X = data, lambda=lambda, epsilon=1e-2, use_ram=TRUE, seed = NULL)
      est <- setNames(lapply(ls(envir=est), mget, envir=attr(unclass(est), '.xData')), ls(envir=est))
      path <-  lapply(seq(length(lambda)), function(i) {
                  tmp <- est$precision_matrices[[1]][[i]][1:p,1:p]
                  diag(tmp) <- 0
                  as(tmp!=0, "lMatrix")
      })
      est$path <- path
      est
  }
  
  # Using Huge GLASSO
  #library(huge)
  #huger <- function(data, lambda) {
    #est  <- huge::huge(data, lambda = lambda, method = "glasso")
    #path <- lapply(seq(length(lambda)), function(i) {
      ## convert precision array to adj list
      #tmp <- est$path[[i]]
      #tmp <- as(as(as(tmp, "lMatrix"), "generalMatrix"), "CsparseMatrix")
      #return(tmp)
    #})
    #est$path <- path
    #est
  #}
    

quicargs <- list(lambda = lambda_path)
#hugeargs <- list(lambda = lambda_path, method = "glasso", verbose = FALSE)
  
library(batchtools)
library(pulsar)
out.p <- my.batch.pulsar(
  data = Hub_data, 
  fun = quicr, 
  fargs = quicargs, 
  rep.num = N,
  thresh = 0.1,
  subsample.ratio = b,
  criterion=c('stars', 'gcd', 'gcd_prior'), 
  lb.stars = TRUE, 
  ub.stars = TRUE, 
  seed = FALSE,
  refit = FALSE,
  fullgcd = FALSE,
  prior_graph = true_graph,
  method = c("spearman", "kendall", "latentcor")
)


# Optimal Index Stars
stars_index <-  my.opt.index(out.p, 'stars')
# Optimal Lambda
best_lambda_stars <- round(lambda_path[stars_index], 3)
# Lower Bound
stars_lb <- out.p[["stars"]][["lb.index"]]
# Upper Bound
stars_ub <- out.p[["stars"]][["ub.index"]]

# Optimal Index Gstars
my.opt.index(out.p, criterion = "gcd") <- my.get.opt.index(out.p, criterion = "gcd")
gstars_index <- my.opt.index(out.p, 'gcd')[[1]]

# Optimal Lambda
best_lambda_gstars <- round(lambda_path[gstars_index], 3)

# Optimal Index PG-stars
my.opt.index(out.p, criterion = "gcd_prior") <- my.get.opt.index(out.p, criterion = "gcd_prior")
pgstars_index <- my.opt.index(out.p, 'gcd_prior')[[1]]
# Optimal Lambda
best_lambda_pgstars <- round(lambda_path[pgstars_index], 3)

fit  <- refit(out.p, criterion = c("stars", "gcd", "gcd_prior"))
## Stars
stars_graph <- fit[["refit"]][["stars"]]
## GStars
gstars_graph <- fit[["refit"]][["gcd"]]
## PGStars
pgstars_graph <- fit[["refit"]][["gcd_prior"]]

## Oracle procedure
oracle_results <- quicr(Hub_data, lambda_path)

#F1-Score as criterium for Oracle
f1_score <- function(actual, predicted) {
if (!inherits(actual, "lgCMatrix") || !inherits(predicted, "lgCMatrix")) {
  stop("Matrices should be of class lgCMatrix")
}

# Calculating TP, FP, and FN using sparse matrix operations
TP <- sum(predicted & actual)
FP <- sum(predicted & !actual)
FN <- sum(!predicted & actual)

# Calculate Precision and Recall
Precision <- ifelse(TP + FP > 0, TP / (TP + FP), 0)
Recall <- ifelse(TP + FN > 0, TP / (TP + FN), 0)

# Calculate F1 Score
f1 <- ifelse(Precision + Recall > 0, 2 * (Precision * Recall) / (Precision + Recall), 0)

return(f1)
}

# Hamming distance as criterium for Oracle
hamming_distance <- function(actual, predicted) {
  sum(tril(predicted) != tril(actual))
}

# F1 score - best lambda Oracle
oracle_index_f1 <- which.max(sapply(1:length(lambda_path), function(j) {
  estimated_graph <- oracle_results$path[[j]]
  f1_score(true_graph, estimated_graph)
}))

# Hamming distance - best lambda Oracle
  oracle_index_hamming <- which.min(sapply(1:length(lambda_path), function(j) {
  estimated_graph <- oracle_results$path[[j]]
  hamming_distance(true_graph, estimated_graph)
}))

best_lambda_oracle_f1 <- round(lambda_path[oracle_index_f1], 3)
best_lambda_oracle_hamming <- round(lambda_path[oracle_index_hamming], 3)

oracle_graph_f1 <- oracle_results$path[[oracle_index_f1]]
oracle_graph_hamming <- oracle_results$path[[oracle_index_hamming]]

compute_metrics <- function(estimated, actual) {

TP <- sum(tril(estimated) & tril(actual))
FP <- sum(tril(estimated) & !tril(actual))
FN <- sum(!tril(estimated) & tril(actual))
Precision <- ifelse(TP + FP > 0, TP / (TP + FP), 0)
Recall <- ifelse(TP + FN > 0, TP / (TP + FN), 0)
F1 <- ifelse(Precision + Recall > 0, 2 * (Precision * Recall) / (Precision + Recall), 0)
# Calculate Hamming distance for the lower triangle only
hamming_distance <- sum(tril(estimated) != tril(actual))

list(F1 = F1, Hamming = hamming_distance)
}

compute_metrics(stars_graph, true_graph)
compute_metrics(gstars_graph, true_graph)
compute_metrics(pgstars_graph, true_graph)
compute_metrics(oracle_graph_f1, true_graph)
compute_metrics(oracle_graph_hamming, true_graph)

plot(out.p, legends = F, scale = F)
```


## Playground
```{r}

graph <- true_graph
graph <- gstars_graph
method = "spearman"


orbind = c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1)+1
five_node = FALSE

nx2 <- .adj2elist(graph)
nx2
n <- length(orbind)
p <- ncol(graph)
orb_count <- orca::count4(nx2) # redundant graphlet degree matrix (gdm) px15
orb_count
  
buffer <- matrix(0, nrow=p-nrow(orb_count), ncol=ncol(orb_count)) # Create empty set up
buffer
orb_count <- rbind(orb_count, buffer)
orb_count

orb_count_normal = rbind(orb_count[,orbind],1)
orb_count_normal[1:5,]



# Example usage
# Assuming orb_count is your matrix
orb_count_noisy <- modify_orb_count(orb_count_normal)
orb_count_noisy[1:5,] 


gcm <- suppressWarnings(cor(orb_count_normal, method = method))
gcm_norm
gcm_rand <- suppressWarnings(cor(orb_count_random, method = method))
gcm_rand

gcv1 <- gcm[upper.tri(gcm)] 
gcv

GCD <- function(gcv1, gcv2){
  res = dist(rbind(gcv1,gcv2))[[1]]
  return(res)
}

GCD(gcv1,gcv2)

dist(rbind(gcv1,gcv2))[[1]]

###################################
## Add noise to orb_count
modify_orb_count <- function(orb_count, noise_prob, noise_strength) {
  # Iterate over each element of the matrix
  for (i in 1:nrow(orb_count)) {
    for (j in 1:ncol(orb_count)) {
      # Decide whether to modify this element
      if (runif(1) < noise_prob) {
        # Randomly choose to add or only subtract if orbit count is greater than 0
        if (runif(1) < 0.5) {
          orb_count[i, j] <- orb_count[i, j] + noise_strength
        } else {
          if (orb_count[i, j] > 0) {
            orb_count[i, j] <- max(0, orb_count[i, j] - noise_strength)  # Ensures orbit count does not go negative
          }
        }
      }
    }
  }
  return(orb_count)
}

#noise = pseudo counts
my.gcvec <- function(graph, method = "spearman", orbind = c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1)+1, 
                     five_node = FALSE, noise = FALSE, noise_prob = 0.5, noise_strength = 1) {
  
  if (length(orbind) < 2) stop("Only one orbit selected, need at least two to calculate graphlet correlations")
  if (any(orbind > 15))   stop("Only 15 orbits, from 4-node graphlets, can be selected")
  if (!method %in% c("kendall", "spearman", "latentcor")) stop("Not supported correlation method is chosen!")
  nx2 <- .adj2elist(graph) # Transform adjacency matrix to nx2 edge matrix
  n <- length(orbind)
  if (ncol(nx2) < 1 || nrow(nx2) < 1) {
      return(rep(0, n*(n-1)/2)) # Return empty vector
  }

  p <- ncol(graph)
  if (five_node == TRUE) { gdm <- orca::count5(nx2)
    } else { gdm <- orca::count4(nx2) # redundant Graphlet Degree Matrix (gdm) px15
  } 
  
  ## expand missing nodes
  buffer <- matrix(0, nrow=p-nrow(gdm), ncol=ncol(gdm)) # Create empty set up
  gdm <- rbind(gdm, buffer) # non-redundant Graphlet Degree Matrix (gdm) px11
  ## warnings here are due to std dev == 0. This almost always occurs for a completely connected
  ## or completely empty graph and can be safely suppressed.
  
  # add one row of 1s to the orbind matrix to overcome std dev == 0 error problem (1s instead of 0s are now non-present edges)
  gdm <- rbind(gdm[,orbind],1)
  
  if (noise == TRUE) {
    ## Add noise to orb_count
    gdm <- modify_orb_count(gdm, noise_prob = noise_prob, noice_strength = noise_strength)
  }
  
  if (method %in% c("kendall", "spearman")){
  #Then calculate the graphlet correlation matrix with method
  gcm <- suppressWarnings(cor(gdm, method = method))
  }
  
  else if (method == "latentcor") {
  gcm <- suppressWarnings(latentcor::latentcor(gdm))  
  }
  
  gcv <- gcm[upper.tri(gcm)] # Create a numeric vector of the upper triangle of gcm
  return(gcv)
}

##############

my.gcd.stability_extended <- function(premerge, thresh, rep.num, p, nlams, prior_graph = NULL) {  
est <- list()

  if (!(prior_graph = NULL)) {
    for(j in 1:length(premerge)){ # j subsamples
      res_tot = c()
      for(i in 1:length(premerge[[j]])){ # i lambda values
        gcv_sub = premerge[[j]][[i]] # gcv of subsample j and lambda i
        gcv_true = my.gcvec(prior_graph) # gcv for true_graph
        res = GCD(gcv_sub, gcv_true) # calculate gcd between (1) gcv of subsample j lambda i and (2) gcv of the true graph
        gcd = c(res_tot, res) # contains all gcd: Numb of gcd = numb of gcv = numb of lams * numb of subsamples N
      }
    
    est$gcv_sub
    est$gcv_true
    est$res
    
    est$merge[[j]] <- gcd # gcd for all j different lambda values (mean over all subsample for each lambda value)
    est$gcv_sub[[j]] = gcv_sub
    est$gcv_true = gcv_true
    }
    
    est$criterion <- "gcd_prior"
  } 
  
  else {
    est$merge <- lapply(1:nlams, function(i) dist(t(sapply(1:rep.num, function(j) premerge[[j]][[i]]))))
    est$criterion <- "gcd"
  }

  est$summary <- vector('numeric', nlams) # set up empty vector
  for (i in 1:nlams) est$summary[i] <- mean(est$merge[[i]]) # fill summary: ith subsamples = mean of ith subsample over all j lambdas
 
  return(est)
} 


#' @importFrom stats dist
#' @keywords internal
gcd.stability <- function(premerge, thresh, rep.num, p, nlams, merge=NULL) { # 6 argumets including nlams
    est <- list()
    
    if (is.null(merge)) {
        est$merge <- lapply(premerge, function(pm) dist(t(sapply(pm, gcvec))))
    } else est$merge <- merge
    

    est$summary <- vector('numeric', nlams) # set up empty vector
    for (i in 1:nlams) est$summary[i] <- mean(est$merge[[i]]) # fill summary i with mean over all subsamples rep.num N
    est$criterion <- "graphlet.stability"
    return(est)
}


```


























## variability plot
```{r}
d_hat <- out.p[["fullgcd"]][["summary"]]
d_hat
D_hat <- out.p[["stars"]][["summary"]]
D_hat
lambda_path
thresh <- out.p[["stars"]][["thresh"]]
#thresh <- 0.1
stars_index
gstars_index
stars_lb
stars_ub

# Load ggplot2
library(ggplot2)

# Normalize d_hat and D_hat to range [0, 1]
normalize <- function(x) {
    (x - min(x)) / (max(x) - min(x))
}

d_hat_norm <- normalize(d_hat)
D_hat_norm <- normalize(D_hat)

d_hat_norm <- d_hat
D_hat_norm <- D_hat

# Create a data frame for plotting
data <- data.frame(
  lambda = lambda_path,
  d_hat = d_hat_norm,
  D_hat = D_hat_norm
)


# Use ggplot to create the plot with additional vertical lines, points, and annotations
Var_path_plot <- ggplot(data) +
  geom_line(aes(x = lambda, y = d_hat, colour = "d_hat"), size = 1.3) +
  geom_line(aes(x = lambda, y = D_hat, colour = "D_hat"), size = 1.3) +
  geom_hline(yintercept = thresh_scaled, linetype = "dashed", color = "black", linewidth = 1) +
  geom_vline(xintercept = lambda_path[stars_lb], linetype = "dotted", color = "gray", size = 1) +
  geom_vline(xintercept = lambda_path[stars_ub], linetype = "dotted", color = "gray", size = 1) +
  geom_point(aes(x = lambda_path[gstars_index], y = d_hat_norm[gstars_index]), color = "#0066cc", size = 2) +
  geom_point(aes(x = lambda_path[stars_index], y = D_hat_norm[stars_index]), color = "#cc3333", size = 2) +
  scale_colour_manual("", 
                      breaks = c("d_hat", "D_hat"),
                      labels = c(expression(italic(bold(hat(d)))), expression(italic(bold(bar(D))))),
                      values = c("d_hat" = "#0066cc", "D_hat" = "#cc3333")) +
  labs(x = expression(italic(bold(lambda[k]))), 
       y = "Variability", 
       title = expression(italic(bold("Normalized Variability vs Lambda")))) +
  theme_minimal() +
  theme(text = element_text(size = 12),  # Adjust text size globally
        axis.title = element_text(size = 14),  # Adjust axis title size
        plot.title = element_text(size = 16, face = "bold.italic"))  # Adjust plot title size and style
             
Var_path_plot




```



## Arriving at gcd
```{r}
#n: number of nodes in a network

## Step 1: Grphlet Degree Vector gdv (1x11)
#skipped


## Step 2: Graphlet Degree Matrix gdm (nx11)
# Converting Hub adjacency matrix to an nx2 edge matrix
true_graph
stars_graph
stars_graph@x
# Count nodes
sum(tril(true_graph))
sum(tril(stars_graph))
true_edge <- which(tril(true_graph, diag = TRUE) == TRUE, arr.ind=TRUE) 
stars_edge <- which(tril(stars_graph, diag = TRUE) == TRUE, arr.ind=TRUE) 
true_edge # 20 hubs*19 connected node = 380/2 = 190 edges
stars_edge
# Sparse alternative from .adj2elist
# (Elist <- .adj2elist(true_graph))
# (Elist <- .adj2elist(stars_graph))
# Count nodes
sum(nrow(true_edge)) # only 190 for p = 200, because diag elements = 0 in adjacency matrix
sum(nrow(stars_edge))
# Function to convert adjacency matrix to nx2 edge matrix, considering only the lower triangle
# convert_to_edge_matrix <- function(adj_matrix) {
#   edges <- which(tril(adj_matrix) == 1, arr.ind = TRUE)
#   edge_matrix <- as.matrix(edges)
#   colnames(edge_matrix) <- c("Node1", "Node2")
#   return(edge_matrix)
# }
# 
# convert_to_edge_matrix(true_graph)

# Determine graphlets via Orca 
library(orca)
true_graphlets <- count4(true_edge)
stars_graphlets <- count4(stars_edge)
true_graphlets 
stars_graphlets
# Select 11 non-redundant orbits
true_non_redun <- true_graphlets[, c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1) +1]
stars_non_redun <- stars_graphlets[, c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1) +1]
true_non_redun #nx11
stars_non_redun
true_non_redun <- rbind(true_non_redun, 1) # Avoid error for std = 0, by setting all zero colomns to 1
stars_non_redun <- rbind(stars_non_redun, 1) 
# Each row in this matrix corresponds to a node in the graph, and each column represents a different orbit type within 4-node graphlets. 
# The values in the matrix are the counts of how many times a node participates in a particular orbit.



## Step 3: Graphlet Correlation Matrix gcm (11x11)
# Calculate the Graphlet Correlation Matrix (GCM) using Spearman's correlation
# Measuring the pairwise correlation between each type of graphlet orbit across all nodes.
gcm_true <- suppressWarnings(cor(true_non_redun, method = "spearman"))
gcm_stars <- cor(stars_non_redun, method = "spearman")
gcm_true 
gcm_stars



## Step 4: Graphlet Correlation Vector gcv
gcv_stars <- gcm_stars[upper.tri(gcm_stars)]
gcv_true <- gcm_true[upper.tri(gcm_true)]
gcv_stars # 1 for fake edges
gcv_true


library(pulsar)
gcv_true_test <- gcvec(true_graph, orbind=c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1)+1)
gcv_stars_test <- gcvec(stars_graph, orbind=c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1)+1)

gcv_true_test
gcv_stars_test



## Step 5: Graphlet Correlation Distance gcd

# Function to calculate total graphlet variability measure over N graphs for each lambda
calculate_total_graphlet_variability <- function(gcv_list) {
  n <- length(gcv_list)
  # Initialize a numeric vector to store the total variability measure for each lambda
  d_N_vector <- numeric(n)
  
  for (k in 1:n) {
    # Compute all pairwise Euclidean distances for the k-th lambda
    pairwise_distances <- combn(gcv_list[[k]], 2, function(gcv_pair) {
      sqrt(sum((gcv_pair[[1]] - gcv_pair[[2]])^2))
    })
    # Calculate the total graphlet variability measure for the k-th lambda
    d_N_vector[k] <- 2 / (n * (n - 1)) * sum(pairwise_distances)
  }
  
  return(d_N_vector)
}

# Example usage:
d_N_vector <- calculate_total_graphlet_variability(gcv_list)
d_N_vector  # This will return the total graphlet variability measure for each lambda



    
  # Graphlet stability path
  d_hat <- out.p$gcd$summary
  d_hat_scaled <- d_hat / max(d_hat)
  
  D_hat
  d_hat
  
  out.p[["gcd"]][["merge"]][[23]] # Already dist of gcv
  (2/(N*(N-1)))*sum(out.p[["gcd"]][["merge"]][[23]]) # d_hat
  
  
  (hui <- fit[["est"]][["path"]][[23]])
  (hui2 <- gcvec(hui, orbind = c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1) + 1))
  (hui3 <- dist(hui2, method = "euclidean"))
  (hui4 <- (2/(N*(N-1)))*sum(hui3))
  
  hui <- triu(fit[["est"]][["path"]][[23]])
  hui
  test <- dist(gcvec(hui, orbind = c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1) + 1))
  test <- dist(gcvec_extended(hui, orbind = c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1) + 1)) # dist of gcv
  test
  (2/(N*(N-1)))*sum(test)

  est$merge <- lapply(premerge, function(pm) dist(t(sapply(pm, gcvec))))

```



