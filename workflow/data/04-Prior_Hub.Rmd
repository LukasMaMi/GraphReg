---
title: "04-Prior_Hub"
output: github_document
---

```{r}
## To Do List:
# Die N mal lambda_k graphs existieren nicht, wenn lb and ub == TRUE. Was existiert ist eine Liste mit N = 2 subsample dann N subsample für den regulierten Pfad und dann wieder N = 2 subsample.
# Es macht keinen Sinn bei den lambda path plots die Ergebnisse mehrer Repetitionen zu betrachten. Weil wenn man den seed nicht setzt, erzeugt man unterschiedliche Graphen über die man nicht den Mittelwert nehmen kann! Und wenn man den Seed setzt erzeugt man wiederholt den gleichen Graphen, und die Varianz ist gleich null.
# Das ist auch der Grund, warum die Pseudo Graphen auf den Lambda path plots (besonders bei prior Information) besser performten, als ihre nicht pseudo counterparts. Es wurden jedes mal unterschiedliche Graphen erzeugt und die pseudo methoden weisen weniger Varianz auf deshalb waren sie über mehrere Repetitionen verschiedener Graphen präziser. 
# Basis GCD Abstand bei PRIOR plots vergrößert sich über dimensions. Bei p = 40 um die 2-3, bei p = 100 schon bei 4-6. 
# Bei set.seed kein Unterschied mehr bei pseudo zu non-pseudo.
```


## Helper Functions
```{r}

#' @keywords internal
.adj2elist <- function(G) {
    if (inherits(G, "sparseMatrix")) {
        G <- Matrix::triu(G, k=1)
        index_i_j <- Matrix::mat2triplet(G)[1:2]
        return(as.data.frame(index_i_j))
    } else {
        p <- ncol(G)
        return(arrayInd(which(as.logical(triu(G))), c(p,p)))
    }
}


# Function to check connectivity and count distinct strings
count_distinct_strings <- function(g, intermediate_nodes) {
  distinct_strings_count <- 0
  visited <- rep(FALSE, vcount(g))
  
  # Check connectivity for each intermediate node
  for (node in intermediate_nodes) {
    if (!visited[node]) {
      visited[node] <- TRUE
      distinct_strings_count <- distinct_strings_count + 1
      
      # Explore connected intermediate nodes
      to_explore <- c(node)
      while (length(to_explore) > 0) {
        current_node <- to_explore[1]
        to_explore <- to_explore[-1]
        
        # Get all unvisited neighbors that are also intermediate nodes
        neighbors <- unlist(neighbors(g, current_node))
        unvisited_intermediate_neighbors <- neighbors[visited[neighbors] == FALSE & neighbors %in% intermediate_nodes]
        
        # Mark them as visited and add to the exploration list
        visited[unvisited_intermediate_neighbors] <- TRUE
        to_explore <- c(to_explore, unvisited_intermediate_neighbors)
      }
    }
  }
  
  return(distinct_strings_count)
}


```



```{r}

ggvec <- function(adj_matrix) { #graphlet ghust vector
  # Convert the adjacency matrix to an igraph graph object
  graph <- igraph::graph_from_adjacency_matrix(adj_matrix, mode = "undirected", diag = FALSE)

  # Transform adjacency matrix to edge list if necessary for graphlet counting
  nx2 <- .adj2elist(adj_matrix)
  
  # Perform graphlet decomposition to get graphlet degree matrix (gdm)
  gdm <- orca::count4(nx2)
  orbind <- c(0, 2, 1, 3) + 1  # Adjusting indices for R (1-based index)
  gdm <- rbind(gdm[,orbind],1) # Correction to avoid stand dev error
  
  # Binary indicator matrix for node-orbit participation
  Pt <- gdm > 0
  Pt <- 1 * Pt

  # Calculate dimensions based on gdm and Pt
  rho_values <- c(
    rho_1 = 1 - 2 * sum(Pt[, "O0"]) / sum(gdm[, "O0"]),
    rho_2 = 1 - sum(Pt[, "O2"] & !Pt[, "O3"]) / sum(Pt[, "O1"] & !Pt[, "O3"]),
    rho_3 = (sum(gdm[, "O1"] * Pt[, "O1"] * (1 - Pt[, "O2"]) * (1 - Pt[, "O3"])) / 
               sum(Pt[, "O1"] * (1 - Pt[, "O2"]) * (1 - Pt[, "O3"]))) / max(gdm[, "O0"]),
    rho_4 = sum(gdm[, "O2"]) / (sum(Pt[, "O2"]) * max(gdm[, "O2"])),
    rho_5 = (0.5 * cov(rank(gdm[, "O1"]), rank(gdm[, "O2"])) / (sd(rank(gdm[, "O1"])) * sd(rank(gdm[, "O2"]))) + 0.5),
    rho_6 = {U2 = gdm[, "O2"] == 1; U3 = gdm[, "O3"] == 0; sum(U2 & U3) / sum(Pt[, "O2"])},
    rho_7 = {n_strings = count_distinct_strings(graph, which(U2 & U3)); 1 - n_strings / sum(U2 & U3)},
    rho_8 = sum(gdm[, "O3"]) / (3 * sum(gdm[, "O2"]) + sum(gdm[, "O3"])),
    rho_9 = 1 - sum(Pt[, "O3"]) / sum(gdm[, "O3"]),
    rho_10 = sum(Pt[, "O3"]) / sum(Pt[, "O0"]),
    rho_11 = {numerator_rho_11 = sum(Pt[, "O3"] * U2); numerator_rho_11 / sum(Pt[, "O3"])},
    rho_12 = (sum(gdm[, "O0"] * Pt[, "O3"]) / sum(Pt[, "O3"])) / max(gdm[, "O0"])
  )
  
  ggv <- matrix(rho_values, nrow = 1, dimnames = list(NULL, names(rho_values)))

  return(ggv)
}

```

```{r}

ggd.stability <- function(premerge, prior_graph, rep.num, p, nlams, use_prior = FALSE, ...) { 
  
    est <- list()

    if (use_prior) {
        ggv_true = my.ggvec(prior_graph, ...) # ggv for true_graph

        est$merge <- vector("list", nlams)
        for (i in 1:nlams) {
            lambda_distances <- numeric(rep.num)
            for (j in 1:rep.num) {
                ggv_sub <- premerge[[j]][[i]] # ggv of subsample j and lambda i
                lambda_distances[j] <- dist(rbind(ggv_sub, ggv_true))[[1]] # calculate gcd
            }
            est$merge[[i]] <- lambda_distances 
        }
    } else { 
        est$merge <- lapply(1:nlams, function(i) dist(t(sapply(1:rep.num, function(j) premerge[[j]][[i]])))) 
    }

    est$summary <- vector('numeric', nlams)
    for (i in 1:nlams) {
        est$summary[i] <- mean(est$merge[[i]]) # Fill summary: ith lambda = mean over all j subsamples 
    }

    return(est)
}

```



```{r}

ggv  <- ggvec(true_graph)
ggv 

gcm <- suppressWarnings(cor(dim_12, method = "spearman"))
gcm


```






```{r}

# Idea: Write the 12 dimesnions in a vector <- Tada: Neuer gcv!
# +1 Line still needed?

graph <- true_graph
dim_12  <- calculate_12_dimensions(true_graph)

adj_matrix <- graph
orbind = c(0, 2, 1, 3)
orbind <- orbind + 1

nx2 <- .adj2elist(graph) # Transform adjacency matrix to nx2 edge matrix
n <- length(orbind)
p <- ncol(graph)
gdm <- orca::count4(nx2)

buffer <- matrix(0, nrow=p-nrow(gdm), ncol=ncol(gdm))
gdm <- rbind(gdm, buffer)
gdm <- rbind(gdm[,orbind],1) 

## Binary indicator P_ij
Pt <- gdm > 0
# Convert logical matrix 'Pt' directly to binary (1/0) without changing its structure
Pt <- 1 * Pt


## Line-surplus coefficient rho_1
sum_O0i <- sum(gdm[, "O0"])  # Sum of all counts for orbit O0 across all nodes
sum_P0i <- sum(Pt[, "O0"])   # Sum of binary variables for orbit O0 across all nodes

# Calculate the line-surplus coefficient rho’_1
rho_prime_1 <- 0.5 * (sum_O0i / sum_P0i) - 1

# Calculate rho_1 by scaling rho_prime_1 between 0 and 1
rho_1 <- 1 - (1 / (rho_prime_1 + 1))

# Alternatively, using the direct formula you provided which uses sums of P0,i and O0,i
rho_1_direct <- 1 - (2 * sum_P0i) / sum_O0i


## Leaf rate rho_2
numerator_rho_2 <- sum(Pt[, "O2"] * (1 - Pt[, "O3"]))
denominator_rho_2 <- sum(Pt[, "O1"] * (1 - Pt[, "O3"]))

# Calculate rho_2
rho_2 <- 1 - (numerator_rho_2 / denominator_rho_2)


## Leaf-base strength rho_3

# Calculate rho_3 prime (rho_3')
# This is the weighted sum of O1 counts for nodes that are leaves (touch O1 but not O2 or O3)
rho_3_prime_numerator <- sum(gdm[, "O1"] * Pt[, "O1"] * (1 - Pt[, "O2"]) * (1 - Pt[, "O3"]))
rho_3_prime_denominator <- sum(Pt[, "O1"] * (1 - Pt[, "O2"]) * (1 - Pt[, "O3"]))

# Avoid division by zero in case there are no such nodes
if (rho_3_prime_denominator == 0) {
    rho_3_prime <- NA  # Not applicable or indeterminate
} else {
    rho_3_prime <- rho_3_prime_numerator / rho_3_prime_denominator
}

# Normalize rho_3 prime by the maximum count in O0
max_O0 <- max(gdm[, "O0"])
rho_3 <- ifelse(max_O0 > 0, rho_3_prime / max_O0, NA)  # Avoid division by zero


## Hub coefficient rho_4

# Calculate rho_4 prime (rho_4')
# This is the average number of times nodes touch O2
rho_4_prime_numerator <- sum(gdm[, "O2"])
rho_4_prime_denominator <- sum(Pt[, "O2"])

# Avoid division by zero
if (rho_4_prime_denominator == 0) {
    rho_4_prime <- NA  # Not applicable or indeterminate
} else {
    rho_4_prime <- rho_4_prime_numerator / rho_4_prime_denominator
}

# Normalize rho_4 prime by the maximum count in O2
max_O2 <- max(gdm[, "O2"])
rho_4 <- ifelse(max_O2 > 0, rho_4_prime / max_O2, NA)  # Avoid division by zero


## Hub-connectivity coefficient rho_5
# Calculate ranks for O1 and O2
rank_O1 <- rank(gdm[, "O1"])
rank_O2 <- rank(gdm[, "O2"])

# Calculate covariance of the rank variables of O1 and O2
cov_rank_O1_O2 <- cov(rank_O1, rank_O2)

# Calculate standard deviations of the rank variables
sigma_rank_O1 <- sd(rank_O1)
sigma_rank_O2 <- sd(rank_O2)

# Calculate rho_5' using the formula for Spearman's rank correlation
rho_5_prime <- cov_rank_O1_O2 / (sigma_rank_O1 * sigma_rank_O2)

# Calculate rho_5
rho_5 <- (rho_5_prime / 2) + 0.5


## String Coefficient rho_6
# Define U2,i based on the condition that O2,i equals 1
U2 <- ifelse(gdm[, "O2"] == 1, 1, 0)

# Define U3,i based on the condition that O3,i equals 0
U3 <- ifelse(gdm[, "O3"] == 0, 1, 0)

# Calculate rho_6 using the given formula
numerator_rho_6 <- sum(U2 * U3)
denominator_rho_6 <- sum(Pt[, "O2"])

# Avoid division by zero in case there are no nodes touching O2
if (denominator_rho_6 == 0) {
    rho_6 <- NA  # Not applicable or indeterminate
} else {
    rho_6 <- numerator_rho_6 / denominator_rho_6
}


## Characteristic String length rho_7
library(igraph)

# Assuming 'adj_matrix' is your adjacency matrix
g <- graph_from_adjacency_matrix(adj_matrix, mode = "undirected", diag = FALSE)

# Intermediate nodes are those for which both U2 and U3 conditions are met
intermediate_nodes <- which(U2 & U3)

# Function to check connectivity and count distinct strings
count_distinct_strings_corrected <- function(g, intermediate_nodes) {
  distinct_strings_count <- 0
  visited <- rep(FALSE, vcount(g))
  
  # Check connectivity for each intermediate node
  for (node in intermediate_nodes) {
    if (!visited[node]) {
      visited[node] <- TRUE
      distinct_strings_count <- distinct_strings_count + 1
      
      # Explore connected intermediate nodes
      to_explore <- c(node)
      while (length(to_explore) > 0) {
        current_node <- to_explore[1]
        to_explore <- to_explore[-1]
        
        # Get all unvisited neighbors that are also intermediate nodes
        neighbors <- unlist(neighbors(g, current_node))
        unvisited_intermediate_neighbors <- neighbors[visited[neighbors] == FALSE & neighbors %in% intermediate_nodes]
        
        # Mark them as visited and add to the exploration list
        visited[unvisited_intermediate_neighbors] <- TRUE
        to_explore <- c(to_explore, unvisited_intermediate_neighbors)
      }
    }
  }
  
  return(distinct_strings_count)
}

# Count the distinct strings with the corrected function
n <- count_distinct_strings_corrected(g, intermediate_nodes)

# Calculate rho_7_prime
if (n > 0) {  # To avoid division by zero
    rho_7_prime <- numerator_rho_6 / n
} else {
    rho_7_prime <- NA  # Not applicable or indeterminate
}

# Calculate rho_7 based on rho_7_prime
if (rho_7_prime > 0) {  # Again to avoid division by zero
    rho_7 <- 1 - (n / numerator_rho_6)
} else {
    rho_7 <- NA  # Not applicable or indeterminate
}


## Triangle rate rho_8
# Calculate the sum of O3,i (number of times nodes are vertices of triangles)
sum_O3i <- sum(gdm[, "O3"])

# Calculate the sum of O2,i (number of G1, or two-node connections, in the network)
sum_O2i <- sum(gdm[, "O2"])

# Calculate the Triangle rate (rho_8)
rho_8 <- sum_O3i / (3 * sum_O2i + sum_O3i)


## Triangle concentration rho_9
# Calculate the sum of P3,i (binary presence of nodes in orbit 3, indicating vertices of triangles)
sum_P3i <- sum(Pt[, "O3"])

# Calculate the sum of O3,i (number of times nodes are vertices of triangles)
sum_O3i <- sum(gdm[, "O3"])

# Calculate the Triangle concentration (rho_9)
rho_9 <- 1 - (sum_P3i / sum_O3i)


## Triangle pervasivness rho_10
# Calculate the sum of P3,i (binary presence of nodes as vertices in triangles)
sum_P3i <- sum(Pt[, "O3"])

# Calculate the sum of P0,i (binary presence of nodes in the network)
sum_P0i <- sum(Pt[, "O0"])

# Calculate the Triangle pervasiveness (rho_10)
rho_10 <- sum_P3i / sum_P0i


## Triangle connectivity
# Calculate the numerator: the number of triangle vertices not connected to other nodes
numerator_rho_11 <- sum(Pt[, "O3"] * U2)

# The denominator is the total number of nodes that are vertices of triangles
denominator_rho_11 <- sum(Pt[, "O3"])

# Calculate the Triangle connectivity (rho_11)
# Avoid division by zero in case there are no triangle vertices
if (denominator_rho_11 == 0) {
    rho_11 <- NA  # Not applicable or indeterminate
} else {
    rho_11 <- numerator_rho_11 / denominator_rho_11
}


## Triangle degree \rho_12
# Calculate the weighted sum of O0,i for nodes that are vertices of triangles
weighted_sum_O0i_P3i <- sum(gdm[, "O0"] * Pt[, "O3"])

# Calculate the total number of nodes that are vertices of triangles
total_P3i <- sum(Pt[, "O3"])

# Calculate rho_12' (average degree of triangle vertices)
rho_12_prime <- ifelse(total_P3i > 0, weighted_sum_O0i_P3i / total_P3i, NA)

# Find the maximum node degree (O0)
max_O0 <- max(gdm[, "O0"])

# Scale rho_12' to range between 0 and 1
rho_12 <- ifelse(max_O0 > 0, rho_12_prime / max_O0, NA)



```


```{r}
library(MASS)
library(Matrix)
library(igraph)
library(huge)
library(pulsar)


generator_geom <- function (n, p, rho, vis, verbose) {
  
  gcinfo(FALSE)
  if (verbose) 
      cat("Generating data from the multivariate normal distribution with the neighboorhod graph structure....")
  

  max_edges <- floor(1 / rho) # Maximum number of non-zero off-diagonal elements per row/column
  
  # Function to calculate the probability based on distance
  prob_geometric <- function(xx1, xx2) {
    distance <- sqrt(sum((xx1 - xx2)^2))
    return((1 / sqrt(2 * pi)) * exp(-4 * distance^2))
  }
  
  # Step 1: Uniformly sample y_1, ..., y_p from a unit square
  x <- runif(p)
  y <- runif(p)
  
  # Step 2: Initialize omega and populate it
  omega <- matrix(0, p, p) # Start with all zeros
  diag(omega) <- 1 # Set the diagonal to 1
  
  for (i in 1:(p-1)) {
    for (j in (i+1):p) {
      xy1 <- c(x[i], y[i])
      xy2 <- c(x[j], y[j])
      prob <- prob_geometric(xy1, xy2)
  
      # Check if adding an edge would exceed the maximum number of edges for either node
      if (runif(1) < prob && sum(omega[i, -i] != 0) < max_edges && sum(omega[-j, j] != 0) < max_edges) {
        omega[i, j] <- omega[j, i] <- rho
      }
    }
  }
  
  #sigma <- solve(omega) 
  sigma = cov2cor(solve(omega)) 
  x <- mvrnorm(n = n, mu = rep(0, p), Sigma = sigma)
  #sigmahat = cov(x)
  sigmahat = cor(x)
  
  theta <- matrix(0, p, p)
    
  # Populate theta by checking non-zero off-diagonal elements of omega
  for (i in 1:p) {
    for (j in 1:p) {
      if (i != j && omega[i, j] != 0) {
        theta[i, j] <- 1
      }
    }
  }
  
  
  ## Adding Null Graph
  theta_null <- matrix(0, p, p) # A p x p matrix of zeros
  # Generate data for the null graph
  data_null <- MASS::mvrnorm(n, mu = rep(0, p), Sigma = diag(p))
  
  if (vis == TRUE) {
      fullfig = par(mfrow = c(2, 2), pty = "s", omi = c(0.3, 
          0.3, 0.3, 0.3), mai = c(0.3, 0.3, 0.3, 0.3))
      fullfig[1] = image(theta, col = gray.colors(256), main = "Adjacency Matrix")
      fullfig[2] = image(sigma, col = gray.colors(256), main = "Covariance Matrix")
      g = graph.adjacency(theta, mode = "undirected", diag = FALSE)
      layout.grid = layout.fruchterman.reingold(g)
      fullfig[3] = plot(g, layout = layout.grid, edge.color = "gray50", 
          vertex.color = "red", vertex.size = 3, vertex.label = NA, 
          main = "Graph Pattern")
      fullfig[4] = image(sigmahat, col = gray.colors(256), 
          main = "Empirical Matrix")
      rm(fullfig, g, layout.grid)
      gc()
  }
  if (verbose) 
      cat("done.\n")
  rm(vis, verbose)
  gc()
  
  sim = list(data = x, sigma = sigma, sigmahat = sigmahat, 
      data_null = data_null, theta_null = as(as(as(theta_null, "lMatrix"), "generalMatrix"), "CsparseMatrix"),
      omega = omega, theta = as(as(as(theta, "lMatrix"), "generalMatrix"), "CsparseMatrix"),
  act_sparsity = sum(theta[lower.tri(theta)]) / (p * (p - 1) / 2)) #-1 because every node can connect to p-1 nodes (discarding diag)
  class(sim) = "sim"
  return(sim)
  
}

n <- 800 # Samples
p <- 40 # Dimensions
b = ifelse(n > 144, (floor(10*sqrt(n)))/n, 0.8) # Size Subsamples (Ratio)
N = 20 # Number of Repetitions
rho <- 0.245 # Strength off-diagonal elements

## Important to set.seed !!!
set.seed(123)
Geom <- generator_geom(n = n, p = p, rho = rho, vis = FALSE, verbose = TRUE)
Geom_data <- Geom$data
true_graph <- Geom$theta
act_sparsity <- Geom$act_sparsity
null_graph <- Geom$theta_null
null_data <- Geom$null_data

maxCov <- getMaxCov(Geom_data)
lambda_path  <- getLamPath(max = maxCov, min = 0.01, len = 30) #,log = TRUE
lambda <- list(lambda=lambda_path)

cat("Condition number:", kappa(Geom$omega), "\n")

#huge.plot(true_graph)

```



```{r}

# Using QUIC
library(BigQuic)
quicr <- function(data, lambda) {
    p <- ncol(data)
    est  <- BigQuic(X = data, lambda=lambda, epsilon=1e-2, use_ram=TRUE, seed = NULL)
    est <- setNames(lapply(ls(envir=est), mget, envir=attr(unclass(est), '.xData')), ls(envir=est))
    path <-  lapply(seq(length(lambda)), function(i) {
                tmp <- est$precision_matrices[[1]][[i]][1:p,1:p]
                diag(tmp) <- 0
                as(tmp!=0, "lMatrix")
    })
    est$path <- path
    est
}

quicargs <- list(lambda = lambda_path)    
  
library(batchtools)
library(pulsar)
#options(mc.cores = 2) #Speed up by setting number of cores available
#options(batchtools.progress=TRUE, batchtools.verbose = TRUE)

time1    <- system.time(    
out.p <- Graphreg(
  data = Geom_data, 
  fun = quicr, 
  fargs = quicargs, 
  rep.num = N,
  thresh = 0.05,
  subsample.ratio = b,
  criterion=c('stars', 'ghust', 'gcd_prior', 'gcd', 'ghust_prior'), 
  lb.stars = TRUE, 
  ub.stars = TRUE, 
  seed = 123,
  refit = TRUE,
  prior_graph = true_graph,
  method = c("spearman", "latentcor", "kendall"),
  use_pseudo_count = TRUE
))

out.p

stop()

for (crit in out.p$criterion) {
  if (startsWith(crit, "gcd_") || startsWith(crit, "ghust")) {
    opt.index(out.p, criterion = crit) <- get.opt.index(out.p, criterion = crit)
  }
}

out.p

refit <- refit.pulsar(out.p)

stop()

plot(out.p, legends = T, show = c("stars", "gcd_pseudo_kendall", "gcd_prior_pseudo_kendall", "ghust", "gcd_prior_kendall", "ghust_prior"))


```


```{r}
my.gcvec <- function(graph, method, orbind, five_node = FALSE, pseudo_count = FALSE, return_gcm = FALSE) {
  
  orbind <- orbind + 1
  if (length(orbind) < 2) stop("Only one orbit selected, need at least two to calculate graphlet correlations")
  if (any(orbind > 15))   stop("Only 15 orbits, from 4-node graphlets, can be selected")
  if (!method %in% c("kendall", "spearman", "latentcor")) stop("Not supported correlation method is chosen!")
  nx2 <- .adj2elist(graph) # Transform adjacency matrix to nx2 edge matrix
  n <- length(orbind)
  if (ncol(nx2) < 1 || nrow(nx2) < 1) {
      return(rep(0, n*(n-1)/2)) # Return empty vector
  }

  p <- ncol(graph)
  if (five_node == TRUE) { gdm <- orca::count5(nx2)
    } else { gdm <- orca::count4(nx2) # redundant Graphlet Degree Matrix (gdm) px15
  } 
  
  ## expand missing nodes
  buffer <- matrix(0, nrow=p-nrow(gdm), ncol=ncol(gdm)) # Create empty set up
  gdm <- rbind(gdm, buffer) # non-redundant Graphlet Degree Matrix (gdm) px11
  ## warnings here are due to std dev == 0. This almost always occurs for a completely connected
  ## or completely empty graph and can be safely suppressed.
  
  if (pseudo_count == TRUE) {
    ## Add pseudo_count to orb_count
    gdm2 <- rbind(gdm[,orbind]) 
    gdm4 <- modify_orb_count(orb_count = gdm, pseudo_count_range = c(0, 0.1))
    print(gdm)
  } else {
    # add one row of 1s to the orbind matrix to overcome std dev == 0 error problem 
    gdm3 <- rbind(gdm[,orbind],1) 
  }
  
  if (method %in% c("kendall", "spearman")){
  #Then calculate the graphlet correlation matrix with method
  gcm <- suppressWarnings(cor(gdm, method = method))
  }
  
  else if (method == "latentcor") {
  gcm <- suppressMessages(latentcor::latentcor(gdm, method = "approx", use.nearPD = FALSE))
  gcm <- gcm$R
  }
  
  gcv <- gcm[upper.tri(gcm)] # Create a numeric vector of the upper triangle of gcm
  
  if (return_gcm == TRUE) {
    return(gcm)
  } else return(gcv)
}


#################
orbind = c(0, 1, 2, 4, 5, 6, 7, 8, 9, 10, 11)

modify_orb_count <- function(orb_count, pseudo_count_range = c(0, 0.1)) {
  # Validate the orbit_count_range input
  if (length(pseudo_count_range) != 2 || pseudo_count_range[1] >= pseudo_count_range[2]) {
    stop("pseudo_count_range must be a vector of two numbers, where the first is less than the second. 
         \n i.e. pseudo_count_range = c(0, 0.1)")
  }

  # Generate a random matrix with the same dimensions as orb_count
  random_matrix <- matrix(runif(nrow(orb_count) * ncol(orb_count), 
                               min = pseudo_count_range[1], 
                               max = pseudo_count_range[2]), 
                          nrow = nrow(orb_count), 
                          ncol = ncol(orb_count))
  
  # Apply conditional logic to add random noise only to the zero elements of orb_count
  modified_orb_count <- mapply(function(orb_elem, random_elem) {
                               if (orb_elem == 0) orb_elem + random_elem else orb_elem
                             }, orb_count, random_matrix)
  
  # Convert the modified_orb_count to a matrix and set the column names
  modified_orb_count_matrix <- matrix(modified_orb_count, nrow = nrow(orb_count), ncol = ncol(orb_count))
  colnames(modified_orb_count_matrix) <- colnames(orb_count)

  return(modified_orb_count_matrix)
}





#' @keywords internal
.adj2elist <- function(G) {
    if (inherits(G, "sparseMatrix")) {
        G <- Matrix::triu(G, k=1)
        index_i_j <- Matrix::mat2triplet(G)[1:2]
        return(as.data.frame(index_i_j))
    } else {
        p <- ncol(G)
        return(arrayInd(which(as.logical(triu(G))), c(p,p)))
    }
}
```






## Playground
```{r}

modify_orb_count <- function(orb_count, pseudo_count_range = c(0, 0.1)) {
  # Validate the orbit_count_range input
  if (length(pseudo_count_range) != 2 || pseudo_count_range[1] >= pseudo_count_range[2]) {
    stop("pseudo_count_range must be a vector of two numbers, where the first is less than the second. 
         \n i.e. pseudo_count_range = c(0, 0.1)")
  }

  # Generate a random matrix with the same dimensions as orb_count
  random_matrix <- matrix(runif(nrow(orb_count) * ncol(orb_count), 
                               min = pseudo_count_range[1], 
                               max = pseudo_count_range[2]), 
                          nrow = nrow(orb_count), 
                          ncol = ncol(orb_count))
  
  # Apply conditional logic to add random noise only to the zero elements of orb_count
  modified_orb_count <- mapply(function(orb_elem, random_elem) {
                               if (orb_elem == 0) orb_elem + random_elem else orb_elem
                             }, orb_count, random_matrix)
  
  # Convert the modified_orb_count to a matrix and set the column names
  modified_orb_count_matrix <- matrix(modified_orb_count, nrow = nrow(orb_count), ncol = ncol(orb_count))
  colnames(modified_orb_count_matrix) <- colnames(orb_count)

  return(modified_orb_count_matrix)
}


# Warum sind die estimated GCM nicht in der Lage die negative Correlation zu messen?
# Mhm warum sind die GCMs bei Yaveroglu et al NUR positiv korreliert aber niemals negativ? 
#Hub: It semms that orb 0 and 1 are konkurieren and 0 and 6 konkurieren
# VERGLEICHE GCM VS GCM PRIOR um herauszufinden, was makante Unterschiede zwischen ihnen sind. Dann vergleiche GCM PRIOR VS GCM PRIOR PSEUDO um herauszufinden was die unterschiede zwischen ihnen sind und warum PSEUDO besser performt!!!
# Bei Verwendung von Noise sollte hinzufügen von einer Reihe Einsen nicht mehr notwendig sein, Überprüfe das!

graph <- true_graph
graph <- gstars_graph
graph <- out.p[["gcd_spearman"]][["refit"]]

orbind = c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1)+1

orbind = c(4, 5, 8, 9, 10, 11, 0, 1, 2, 6, 7)+1

# orbind = c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1)+1
# orbind = c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1)+1
# orbind = c(0, 2, 5, 7, 8, 10, 11, 6, 9, 4, 1)+1


nx2 <- .adj2elist(graph)
nx2 
n <- length(orbind)
p <- ncol(graph)
orb_count <- orca::count4(nx2) # redundant graphlet degree matrix (gdm) px15
orb_count
buffer <- matrix(0, nrow=p-nrow(orb_count), ncol=ncol(orb_count)) # Create empty set up
buffer
orb_count <- rbind(orb_count, buffer)
orb_count 
orb_count_normal = rbind(orb_count[,orbind],1)
orb_count_normal

orb_count_noise <- modify_orb_count(orb_count_normal, pseudo_count_range = pseudo_count_range)
orb_count_noise

gcm <- suppressWarnings(cor(orb_count_normal, method = "spearman"))
gcm <- suppressMessages(latentcor::latentcor(orb_count_normal, method = "approx", use.nearPD = TRUE))
gcm 

corrplot::corrplot(gcm)

gcv2 <- gcm[upper.tri(gcm)]
gcv1

gcd <- dist(gcv1, gcv2)
gcd






test <- my.gcvec(true_graph, method = "spearman", orbind = orbind, five_node = FALSE,
                                                         pseudo_count = TRUE, return_gcm = TRUE, 
                                                         pseudo_count_range = pseudo_count_range)

test


GCD <- function(gcv1, gcv2){
  res = dist(rbind(gcv1,gcv2))[[1]]
  return(res)
}

GCD(gcv1,gcv2)





```




























