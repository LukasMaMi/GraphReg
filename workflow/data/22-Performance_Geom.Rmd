---
title: "22-Performance_Geom"
output: github_document
---

```{r}

# Load session settings
#Geom_setting_path <- "C:/R Projekte/StARS_Simulations/workflow/Storage_Settings/"
Geom_setting_path <- "/Users/bropc/Documents/LMU/Master Statistics and Data Science/Masterarbeit/R Master/StARS_Simulations/workflow/Storage_Settings/"
Geom_settings_file <- file.path(Geom_setting_path, "Geom_settings.RData")
load(Geom_settings_file)

#dir_path3 <- "C:/R Projekte/StARS_Simulations/workflow/Storage_Performance_Geom"
dir_path3 <- "/Users/bropc/Documents/LMU/Master Statistics and Data Science/Masterarbeit/R Master/StARS_Simulations/workflow/Storage_Performance_Geom"

# Function to generate file names
get_filename <- function(config, rep, prefix = "Geom") {
  sprintf("%s_rep_%d_n_%d_p_%d.RData", prefix, rep, config$n, config$p)
}


# Function to calculate F1-score and Hamming distance
compute_metrics <- function(true_graph, est_graph) {

  TP <- sum(est_graph & true_graph)
  FP <- sum(est_graph & !true_graph)
  FN <- sum(!est_graph & true_graph)
  Precision <- ifelse(TP + FP > 0, TP / (TP + FP), 0)
  Recall <- ifelse(TP + FN > 0, TP / (TP + FN), 0)
  F1 <- ifelse(Precision + Recall > 0, 2 * (Precision * Recall) / (Precision + Recall), 0)
  hamming_distance <- sum(est_graph != true_graph)
  
  list(F1 = F1, Hamming = hamming_distance)
}

# Initialize list to store results for each configuration
config_results <- list()

# Loop over configurations and repetitions
for(cfg in configs) {
  cfg_key <- paste("n", cfg$n, "p", cfg$p, sep="_")
  individual_results <- list()
  aggregated_metrics <- list(stars = NULL, gstars = NULL, oracle = NULL)
  
  lambda_values <- list(stars = numeric(), gstars = numeric(), oracle = numeric())
  sparsity_values <- list(stars = numeric(), gstars = numeric(), oracle = numeric())

  # Loop over repetitions for the current configuration
  for(rep in 1:num_repetitions) {
    load(paste0(dir_path2, "/", get_filename(cfg, rep, "estimation")))

    # Compute metrics
    stars_metrics <- compute_metrics(stars_graph, true_graph)
    gstars_metrics <- compute_metrics(gstars_graph, true_graph)
    oracle_metrics <- compute_metrics(oracle_graph, true_graph)

    # Additional metrics: lambda values and sparsity
    additional_metrics <- list(
      stars_lambda = best_lambda_stars,
      gstars_lambda = best_lambda_gstars,
      oracle_lambda = best_lambda_oracle,
      true_sparsity = act_sparsity,
      stars_sparsity = act_sparsity_stars,
      gstars_sparsity = act_sparsity_gstars,
      oracle_sparsity = act_sparsity_oracle,
      stars_index = stars_index,
      stars_lb = stars_lb,
      stars_ub = stars_ub,
      gstars_index = gstars_index,
      oracle_index = oracle_index
    )

    # Store individual results
    individual_results[[paste("Rep", rep)]] <- list(
      Stars = stars_metrics,
      GStars = gstars_metrics,
      Oracle = oracle_metrics,
      Additional = additional_metrics
    )

    # Accumulate metrics for aggregation
    # Convert to numeric and convert to a matrix format
    aggregated_metrics$stars <- rbind(aggregated_metrics$stars, as.numeric(unlist(stars_metrics)))
    aggregated_metrics$gstars <- rbind(aggregated_metrics$gstars, as.numeric(unlist(gstars_metrics)))
    aggregated_metrics$oracle <- rbind(aggregated_metrics$oracle, as.numeric(unlist(oracle_metrics)))
    
     # Accumulate lambda and sparsity values
    lambda_values$stars <- c(lambda_values$stars, additional_metrics$stars_lambda)
    sparsity_values$stars <- c(sparsity_values$stars, additional_metrics$stars_sparsity)
    lambda_values$gstars <- c(lambda_values$gstars, additional_metrics$gstars_lambda)
    sparsity_values$gstars <- c(sparsity_values$gstars, additional_metrics$gstars_sparsity)
    lambda_values$oracle <- c(lambda_values$oracle, additional_metrics$oracle_lambda)
    sparsity_values$oracle <- c(sparsity_values$oracle, additional_metrics$oracle_sparsity)
  }

  # Calculate means and confidence intervals
  calc_mean_ci <- function(data, lambda, sparsity) {
    mean_data <- colMeans(data, na.rm = TRUE)
    ci_data <- apply(data, 2, function(x) quantile(x, probs = c(0.025, 0.975), na.rm = TRUE))
    mean_lambda <- mean(lambda, na.rm = TRUE)
    mean_sparsity <- mean(sparsity, na.rm = TRUE)
    list(Mean = mean_data, CI = ci_data, Lambda = mean_lambda, Sparsity = mean_sparsity)
  }
  
  # Include mean lambda and sparsity in aggregated metrics
  aggregated_metrics$stars <- calc_mean_ci(aggregated_metrics$stars, lambda_values$stars, sparsity_values$stars)
  aggregated_metrics$gstars <- calc_mean_ci(aggregated_metrics$gstars, lambda_values$gstars, sparsity_values$gstars)
  aggregated_metrics$oracle <- calc_mean_ci(aggregated_metrics$oracle, lambda_values$oracle, sparsity_values$oracle)

  # Store results for the current configuration
  config_results[[cfg_key]] <- list(
    Individual = individual_results,
    Aggregated = aggregated_metrics
  )
}

# Save the results to a file
performance_filename <- "all_performance_results.RData"
save(config_results, file = file.path(dir_path3, performance_filename))

cat("Performance metrics for each configuration and repetition calculated and saved!\n")

```
