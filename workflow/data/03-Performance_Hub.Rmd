---
title: "03-Performance_Hub"
output: github_document
---

```{r}

# Define the directory for estimated results
dir_path3 <- "C:/R Projekte/StARS_Simulations/workflow/Storage_Performance_Hub"

# 1. Generate list of file names based on configs
get_filename <- function(config, rep, prefix = "hub") {
  return(sprintf("%s_rep_%d_n_%d_p_%d.RData", prefix, rep, config$n, config$p))
}

# Function to calculate F1-score and Jaccard index
compute_metrics <- function(est_graph, true_graph) {
  TP <- sum(est_graph == 1 & true_graph == 1)
  FP <- sum(est_graph == 1 & true_graph == 0)
  FN <- sum(est_graph == 0 & true_graph == 1)

  precision <- TP / (TP + FP)
  recall <- TP / (TP + FN)
  F1 <- 2 * (precision * recall) / (precision + recall)
  jaccard_index <- TP / (TP + FP + FN)

  return(list(F1 = F1, Jaccard = jaccard_index))
}

# Loop over configurations
results <- list()

for(cfg in configs) {
  
  # Initialize lists to store metrics for each repetition
  stars_results <- list()
  gstars_results <- list()
  oracle_results <- list()

  for(rep in 1:num_repetitions) {
    
    # Load the simulated data
    sim_filename <- paste0(dir_path, "/hub_rep_", rep, "_n_", cfg$n, "_p_", cfg$p, ".RData")
    load(sim_filename)
    
    # Load the results
    result_filename <- get_filename(cfg, rep, prefix="estimation")
    load(paste0(dir_path2, "/", result_filename))

    # Compute metrics for each method
    stars_results[[rep]] <- compute_metrics(stars_graph, true_graph)
    gstars_results[[rep]] <- compute_metrics(gstars_graph, true_graph)
    oracle_results[[rep]] <- compute_metrics(oracle_graph, true_graph)
  }

  # Compute mean and confidence intervals
  cfg_key <- paste("n", cfg$n, "p", cfg$p, sep="_")
  results[[cfg_key]] <- list(
    Stars = list(
      Mean_F1 = mean(sapply(stars_results, function(x) x$F1)),
      CI_F1 = quantile(sapply(stars_results, function(x) x$F1), probs = c(0.025, 0.975)),
      Mean_Jaccard = mean(sapply(stars_results, function(x) x$Jaccard)),
      CI_Jaccard = quantile(sapply(stars_results, function(x) x$Jaccard), probs = c(0.025, 0.975))
    ),
    GStars = list(
      Mean_F1 = mean(sapply(gstars_results, function(x) x$F1)),
      CI_F1 = quantile(sapply(gstars_results, function(x) x$F1), probs = c(0.025, 0.975)),
      Mean_Jaccard = mean(sapply(gstars_results, function(x) x$Jaccard)),
      CI_Jaccard = quantile(sapply(gstars_results, function(x) x$Jaccard), probs = c(0.025, 0.975))
    ),
    Oracle = list(
      Mean_F1 = mean(sapply(oracle_results, function(x) x$F1)),
      CI_F1 = quantile(sapply(oracle_results, function(x) x$F1), probs = c(0.025, 0.975)),
      Mean_Jaccard = mean(sapply(oracle_results, function(x) x$Jaccard)),
      CI_Jaccard = quantile(sapply(oracle_results, function(x) x$Jaccard), probs = c(0.025, 0.975))
    )
  )
}

# Save metrics to the directory
save(results, file=paste0(dir_path3, "/performance_metrics.RData"))

print("Performance metrics calculated and saved!")

```

## Check
```{r}
print(results)

```

